[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "WarningDraft Version\n\n\n\nThis document is a work in progress and not the final version.\nThis Handbook on Data and Knowledge Publication in the Soil Domain equips researchers with essential strategies and practical steps to publish soil-related data in a way that maximizes its value, visibility, and reuse across disciplines. Rooted in the FAIR (Findable, Accessible, Interoperable, Reusable) principles, it responds to the growing need for coherent, transparent, and collaborative soil science data practice. By ensuring that datasets and knowledge assets are well documented, richly described, and sustainably archived, the guidelines help overcome fragmentation and enhance research impact.\nThe handbook bridges conceptual frameworks and hands-on workflows, from choosing repositories to metadata design and licensing. It supports researchers throughout the data lifecycle — from planning and management to publication and long-term stewardship. Emphasis is placed on community standards, persistent identifiers (such as DOIs), and machine-actionable metadata to unlock broader interoperability. By following these guidelines, soil scientists will contribute to a more open, efficiently shared, and more robust scientific record. The principles also underpin compliance with funding policies and international research infrastructures. Whether producing field measurements, models, or derived data products, authors will find clear direction here. This handbook encourages a culture of responsible data stewardship that fosters innovation, reproducibility, and cross-project synthesis. Welcome to a resource designed to make your soil data FAIR, valuable and enduring."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Home",
    "section": "Contents:",
    "text": "Contents:\n\nGuideline Overview A comprehensive overview of the FAIR principles and their application in soil science, including best practices for data management, metadata standards, and licensing.\nSet of use cases in FAIR sharing A collection of practical examples demonstrating how to implement FAIR principles in various soil science contexts, such as field data, remote sensing, and modeling outputs."
  },
  {
    "objectID": "index.html#presentations-based-on-the-handbook",
    "href": "index.html#presentations-based-on-the-handbook",
    "title": "Home",
    "section": "Presentations based on the handbook",
    "text": "Presentations based on the handbook\nThe content of the handbook is regularly presented at relevent events. Slides of these presentations are readily available.\n\nFAIR publications using Zenodo, application to observational data and websites"
  },
  {
    "objectID": "use-cases/tabular-data.html",
    "href": "use-cases/tabular-data.html",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "",
    "text": "Soil observation data is often shared in tabular formats, such as Excel, Access, CSV, Sqlite. This document provides a strategy on how to increase the interoperability of tabular datasets.\nWhich phenomena are observed, using which procedure, in which unit? These aspects are often explained in a related report or readme.txt document. Unfortunately this information can not easily be read by machines. To increase findability and interoperability of tabular datasets this document provides a strategy on how to encode this information using a standardised approach. The document also provides more generic information about capturing data in tabular formats."
  },
  {
    "objectID": "use-cases/tabular-data.html#workflow",
    "href": "use-cases/tabular-data.html#workflow",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "Workflow",
    "text": "Workflow\nStep 1: Annotate a CSV using CSV on the Web (CSVW)\n\nCreate a CSVW metadata JSON file, describing the CSV structure and semantics. Tools, such as CoW exist to create a minimal context file.\nUse the CSVW standard to define:\n\nColumn datatypes\n\nColumn meanings using URIs from ontologies\n\nPrimary keys\n\nRow-level subject generation pattern (e.g., http://example.org/soilobs/{Sample.ID})"
  },
  {
    "objectID": "use-cases/tabular-data.html#step-2-use-standard-vocabulariesontologies",
    "href": "use-cases/tabular-data.html#step-2-use-standard-vocabulariesontologies",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "Step 2: Use Standard Vocabularies/Ontologies",
    "text": "Step 2: Use Standard Vocabularies/Ontologies\nAlign the properties and structure to well-known vocabularies:\n\nGeo (WKT, GeoSPARQL) for location\n\nQUDT or OM (Ontology of Units of Measure) for measurement values\n\nSOSA/SSN (Semantic Sensor Network Ontology) for observations\n\nPROV for provenance (e.g., who collected the sample and when)"
  },
  {
    "objectID": "use-cases/tabular-data.html#step-3-convert-to-rdf-or-json-ld",
    "href": "use-cases/tabular-data.html#step-3-convert-to-rdf-or-json-ld",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "Step 3: Convert to RDF or JSON-LD",
    "text": "Step 3: Convert to RDF or JSON-LD\nUse tools like:\n\ncsvwlib python\nRDFTabular - java\ncsv2rdf - r\n\nConvert annotated CSV to RDF Turtle or JSON-LD for publishing."
  },
  {
    "objectID": "use-cases/tabular-data.html#step-4-validate-the-data",
    "href": "use-cases/tabular-data.html#step-4-validate-the-data",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "Step 4: Validate the Data",
    "text": "Step 4: Validate the Data\nYou can use SHACL or schema.org validator to validate the generated RDF."
  },
  {
    "objectID": "use-cases/tabular-data.html#step-5-publish-the-data",
    "href": "use-cases/tabular-data.html#step-5-publish-the-data",
    "title": "Sharing Soil Observation Data on the Web Using tabular formats",
    "section": "Step 5: Publish the Data",
    "text": "Step 5: Publish the Data\n\nHost the original CSV, the context file and the RDF/JSON-LD on a repository (zenodo/dataverse)\nAs part of the publication, provide sensible metadata (keywords, authors, license, dates, references)"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html",
    "href": "use-cases/soil-property-indicator-maps.html",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "This document provides best practices for ensuring that gridded soil products—such as predicted distributions of soil properties or health indicators across space and/or time—are published according to the FAIR principles: Findable, Accessible, Interoperable, and Reusable. These products are often generated using machine learning (commonly Random Forest), based on point observations and environmental co-variates. Proper documentation of methods, inputs, uncertainties, and usage limitations is essential.\n\n\n\n\n\nEach dataset used to train or validate the model must be referenced and, where possible, shared or openly cited.\nMinimum requirements:\n\nPersistent identifier (e.g. DOI, accession number)\nSampling design overview (source campaigns or databases)\nAttributes measured (e.g. SOC, pH, bulk density, biological indicators)\nSpatial coordinate reference system\nTemporal coverage (when collected)\nLicensing and access conditions\nLink to associated metadata\n\nIf privacy or license restrictions prevent data sharing, reference the repository or publication where the data can be requested.\n\n\n\nAll co-variates used to fit the model must be properly documented to ensure reproducibility.\nKey metadata:\n\nDataset name and version\nDescription (e.g. climate, terrain, remote sensing, parent material)\nSpatial resolution and coordinate reference system\nTemporal coverage (for time-specific variables)\nSource and access link (URL, DOI, repository)\nLicense and usage constraints\n\n\n\n\n\n\n\nClearly state:\n\nAlgorithm used (e.g. Random Forest)\nSoftware or library (e.g. scikit-learn, ranger, caret, R randomForest)\nVersion number\nComputing environment details (OS, language version, dependencies)\n\n\n\n\nDocument:\n\nNumber of trees\nNode size, mtry/feature selection approach\nCross-validation or validation method\nTrain/test split or resampling strategy\nFeature importance metrics, if calculated\n\nInclude or link to:\n\nScripts or notebooks used for training and prediction\nLogs of training runs or configuration files\n\n\n\n\nProvide relevant fit metrics, such as:\n\nRMSE, MAE, R² (for continuous properties)\nConfusion matrix, kappa, AUROC (if classification)\nSpatial or temporal cross-validation\nAny external validation datasets\n\n\n\n\n\n\n\nFor each soil map (raster or vector), make sure metadata includes:\n\nProduct title and abstract\nTarget property or indicator (select from common vocabularies)\nSpatial resolution\nTemporal reference (year, season, baseline or scenario)\nSpatial extent and coordinate reference system\nVersion or edition number\nContact information or responsible organization\n\n\n\n\nInclude references to:\n\nPoint datasets (with identifiers)\nCo-variates (with versions and licenses)\nModel description, parameters, and performance\n\nThese should be captured in metadata fields (e.g. ISO 19115, Dublin Core, DCAT, or INSPIRE-compliant formats).\n\n\n\nPreferred FAIR-friendly formats:\n\nRaster: GeoTIFF, NetCDF, Cloud-Optimized GeoTIFF\nVector: GeoPackage, shapefile (as fallback), GeoJSON\nMetadata: XML, JSON, or YAML aligned with standards\nModel Docs: PDF, Markdown, or linked code repository\n\n\n\n\n\n\n\nPublish one or more of the following:\n\nPixel-level uncertainty or prediction interval maps\nStandard error or variance layers\nValidation residual surfaces\nConfidence class maps\n\n\n\n\nDocument:\n\nSpatial or temporal domains for which predictions are valid\nKnown gaps or biases (e.g. underrepresented soil types or regions)\nLimitations due to input data density or co-variate quality\nScale constraints (e.g. not suitable for farm-level decisions)\n\nInclude a clear statement on:\n\nAppropriate applications (e.g. regional modeling, national planning)\nInappropriate uses (e.g. site-specific legal or regulatory decisions)\n\n\n\n\nSpecify:\n\nLicense type (e.g. CC-BY, CC0, ODbL)\nAny attribution requirements\nCitation instructions\n\n\n\n\n\n\n\nDeposit map layers and accompanying metadata in a FAIR-compliant repository:\n\nExamples: Zenodo, Figshare, institutional data portals, INSPIRE-compliant nodes\nProvide persistent identifiers (e.g. DOI)\n\n\n\n\nPublish with:\n\nStandardized coordinate reference systems\nOpen geospatial formats\nMetadata standards (ISO 19115, DCAT, INSPIRE)\nOptional API or OGC services (WMS/WCS/WFS/GeoTIFF over HTTP)\n\n\n\n\nWhere feasible, include or link to:\n\nModel code and environment specifications\nData preparation workflows\nDocumentation for rerunning or updating predictions\n\n\n\n\n\nTrack and record:\n\nVersion numbers and release dates\nChanges in point data, co-variates, or model parameters\nDeprecated or superseded versions\nArchive of previous editions for reference\n\n\n\n\nProvide a formatted citation that includes:\n\nTitle of the dataset\nVersion\nAuthors or organizations\nYear\nDOI or persistent link\n\nIf the map is derived from external datasets, include recommended citations for each.\n\n\n\n\n\n\nComponent\nFAIR Requirement\n\n\n\n\nPoint data\nCited, licensed, persistent ID\n\n\nCo-variates\nVersioned, referenced, licensed\n\n\nModel details\nAlgorithm, parameters, validation, code ref\n\n\nMap product\nGeospatial metadata, DOI, standardized format\n\n\nUncertainty\nPublished or referenced, explained\n\n\nUsage limits\nClearly documented\n\n\nLicensing\nExplicit and machine-readable\n\n\nVersioning\nTraceable and archived\n\n\n\n\n\n\nBy adhering to these guidelines, soil map products become not only publishable but also traceable, interoperable, and reusable across projects, regions, and time. This ensures scientific transparency, policy relevance, and long-term value of soil information systems."
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#purpose",
    "href": "use-cases/soil-property-indicator-maps.html#purpose",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "This document provides best practices for ensuring that gridded soil products—such as predicted distributions of soil properties or health indicators across space and/or time—are published according to the FAIR principles: Findable, Accessible, Interoperable, and Reusable. These products are often generated using machine learning (commonly Random Forest), based on point observations and environmental co-variates. Proper documentation of methods, inputs, uncertainties, and usage limitations is essential."
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#core-components-of-soil-map-products",
    "href": "use-cases/soil-property-indicator-maps.html#core-components-of-soil-map-products",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Each dataset used to train or validate the model must be referenced and, where possible, shared or openly cited.\nMinimum requirements:\n\nPersistent identifier (e.g. DOI, accession number)\nSampling design overview (source campaigns or databases)\nAttributes measured (e.g. SOC, pH, bulk density, biological indicators)\nSpatial coordinate reference system\nTemporal coverage (when collected)\nLicensing and access conditions\nLink to associated metadata\n\nIf privacy or license restrictions prevent data sharing, reference the repository or publication where the data can be requested.\n\n\n\nAll co-variates used to fit the model must be properly documented to ensure reproducibility.\nKey metadata:\n\nDataset name and version\nDescription (e.g. climate, terrain, remote sensing, parent material)\nSpatial resolution and coordinate reference system\nTemporal coverage (for time-specific variables)\nSource and access link (URL, DOI, repository)\nLicense and usage constraints"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#modeling-framework-documentation",
    "href": "use-cases/soil-property-indicator-maps.html#modeling-framework-documentation",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Clearly state:\n\nAlgorithm used (e.g. Random Forest)\nSoftware or library (e.g. scikit-learn, ranger, caret, R randomForest)\nVersion number\nComputing environment details (OS, language version, dependencies)\n\n\n\n\nDocument:\n\nNumber of trees\nNode size, mtry/feature selection approach\nCross-validation or validation method\nTrain/test split or resampling strategy\nFeature importance metrics, if calculated\n\nInclude or link to:\n\nScripts or notebooks used for training and prediction\nLogs of training runs or configuration files\n\n\n\n\nProvide relevant fit metrics, such as:\n\nRMSE, MAE, R² (for continuous properties)\nConfusion matrix, kappa, AUROC (if classification)\nSpatial or temporal cross-validation\nAny external validation datasets"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#product-metadata-and-publication-format",
    "href": "use-cases/soil-property-indicator-maps.html#product-metadata-and-publication-format",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "For each soil map (raster or vector), make sure metadata includes:\n\nProduct title and abstract\nTarget property or indicator (select from common vocabularies)\nSpatial resolution\nTemporal reference (year, season, baseline or scenario)\nSpatial extent and coordinate reference system\nVersion or edition number\nContact information or responsible organization\n\n\n\n\nInclude references to:\n\nPoint datasets (with identifiers)\nCo-variates (with versions and licenses)\nModel description, parameters, and performance\n\nThese should be captured in metadata fields (e.g. ISO 19115, Dublin Core, DCAT, or INSPIRE-compliant formats).\n\n\n\nPreferred FAIR-friendly formats:\n\nRaster: GeoTIFF, NetCDF, Cloud-Optimized GeoTIFF\nVector: GeoPackage, shapefile (as fallback), GeoJSON\nMetadata: XML, JSON, or YAML aligned with standards\nModel Docs: PDF, Markdown, or linked code repository"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#uncertainty-and-usage-limitations",
    "href": "use-cases/soil-property-indicator-maps.html#uncertainty-and-usage-limitations",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Publish one or more of the following:\n\nPixel-level uncertainty or prediction interval maps\nStandard error or variance layers\nValidation residual surfaces\nConfidence class maps\n\n\n\n\nDocument:\n\nSpatial or temporal domains for which predictions are valid\nKnown gaps or biases (e.g. underrepresented soil types or regions)\nLimitations due to input data density or co-variate quality\nScale constraints (e.g. not suitable for farm-level decisions)\n\nInclude a clear statement on:\n\nAppropriate applications (e.g. regional modeling, national planning)\nInappropriate uses (e.g. site-specific legal or regulatory decisions)\n\n\n\n\nSpecify:\n\nLicense type (e.g. CC-BY, CC0, ODbL)\nAny attribution requirements\nCitation instructions"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#accessibility-and-reuse",
    "href": "use-cases/soil-property-indicator-maps.html#accessibility-and-reuse",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Deposit map layers and accompanying metadata in a FAIR-compliant repository:\n\nExamples: Zenodo, Figshare, institutional data portals, INSPIRE-compliant nodes\nProvide persistent identifiers (e.g. DOI)\n\n\n\n\nPublish with:\n\nStandardized coordinate reference systems\nOpen geospatial formats\nMetadata standards (ISO 19115, DCAT, INSPIRE)\nOptional API or OGC services (WMS/WCS/WFS/GeoTIFF over HTTP)\n\n\n\n\nWhere feasible, include or link to:\n\nModel code and environment specifications\nData preparation workflows\nDocumentation for rerunning or updating predictions"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#versioning-and-updates",
    "href": "use-cases/soil-property-indicator-maps.html#versioning-and-updates",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Track and record:\n\nVersion numbers and release dates\nChanges in point data, co-variates, or model parameters\nDeprecated or superseded versions\nArchive of previous editions for reference"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#citation-and-acknowledgment",
    "href": "use-cases/soil-property-indicator-maps.html#citation-and-acknowledgment",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Provide a formatted citation that includes:\n\nTitle of the dataset\nVersion\nAuthors or organizations\nYear\nDOI or persistent link\n\nIf the map is derived from external datasets, include recommended citations for each."
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#summary-checklist",
    "href": "use-cases/soil-property-indicator-maps.html#summary-checklist",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "Component\nFAIR Requirement\n\n\n\n\nPoint data\nCited, licensed, persistent ID\n\n\nCo-variates\nVersioned, referenced, licensed\n\n\nModel details\nAlgorithm, parameters, validation, code ref\n\n\nMap product\nGeospatial metadata, DOI, standardized format\n\n\nUncertainty\nPublished or referenced, explained\n\n\nUsage limits\nClearly documented\n\n\nLicensing\nExplicit and machine-readable\n\n\nVersioning\nTraceable and archived"
  },
  {
    "objectID": "use-cases/soil-property-indicator-maps.html#conclusion",
    "href": "use-cases/soil-property-indicator-maps.html#conclusion",
    "title": "Guidance for FAIR Publication of Soil Property and Soil Health Indicator Maps",
    "section": "",
    "text": "By adhering to these guidelines, soil map products become not only publishable but also traceable, interoperable, and reusable across projects, regions, and time. This ensures scientific transparency, policy relevance, and long-term value of soil information systems."
  },
  {
    "objectID": "use-cases/inspire-xml.html",
    "href": "use-cases/inspire-xml.html",
    "title": "INSPIRE XML",
    "section": "",
    "text": "Publish two complementary outputs:\n\nAn INSPIRE-compliant GML dataset + OGC services (WFS/WMS/Download), encoded using the INSPIRE Soil application schema (GML/XSD) and the associated technical guidelines. This enables interoperability with European infrastructures and geospatial clients. (inspire-mif.github.io)\nA repository-grade package (RDF metadata / DCAT or Dublin Core + dataset landing page + DOI) containing the canonical INSPIRE GML dump, service endpoints, validation report and human-friendly exports (CSV, GeoPackage). Index the dataset in your institutional repository so non-GIS consumers can find and cite it.\n\nTogether these make your soil observations discoverable, machine-actionable and citable across both geospatial and academic ecosystems. (inspire-mif.github.io)"
  },
  {
    "objectID": "use-cases/inspire-xml.html#map-your-domain-to-inspire-feature-types",
    "href": "use-cases/inspire-xml.html#map-your-domain-to-inspire-feature-types",
    "title": "INSPIRE XML",
    "section": "Map your domain to INSPIRE feature types",
    "text": "Map your domain to INSPIRE feature types\nCore INSPIRE Soil feature types (conceptual equivalents you should map to) include (see INSPIRE docs for exact class names / UML):\n\nSoilBody / SoilProfile — macroscopic bodies / profiles. Map your site/profile/horizon constructs here. (inspire-mif.github.io)\nSoilObservation / SoilRelatedProperty — thematic observations and derived properties (may use ISO 19156/OM for measurement representation). (inspire-mif.github.io)\nSamplingFeatures / Sample — link physical samples to SoilProfile and SoilBody features.\nSpatial features / geometries — use ISO/OGC geometry encodings in GML (boundedBy, geometryMember).\n\nCreate a mapping table: your source column → INSPIRE UML attribute / association → target GML element / XSD element. (I can generate a starter mapping table for your CSV if you paste 3–8 sample rows.)"
  },
  {
    "objectID": "use-cases/inspire-xml.html#units-vocabularies-controlled-lists",
    "href": "use-cases/inspire-xml.html#units-vocabularies-controlled-lists",
    "title": "INSPIRE XML",
    "section": "Units, vocabularies & controlled lists",
    "text": "Units, vocabularies & controlled lists\n\nNormalise measurement units and codes before encoding. INSPIRE expects consistent vocabularies and enumerations where the application schema prescribes them. Where INSPIRE uses code lists, re-use those or document your extension. (inspire-mif.github.io)"
  },
  {
    "objectID": "use-cases/inspire-xml.html#gml-encoding-recommendations",
    "href": "use-cases/inspire-xml.html#gml-encoding-recommendations",
    "title": "INSPIRE XML",
    "section": "GML encoding recommendations",
    "text": "GML encoding recommendations\n\nUse the INSPIRE application schema XSD (provided with the technical guidelines) to encode feature instances. Validate generated GML against those XSDs. (inspire-mif.github.io)\nRepresent observations that are temporal or measured quantities following ISO 19156 (Observations & Measurements) where possible (INSPIRE guidance refers to O&M patterns). (inspire-mif.github.io)"
  },
  {
    "objectID": "use-cases/inspire-xml.html#transformation-tools-practical-tips",
    "href": "use-cases/inspire-xml.html#transformation-tools-practical-tips",
    "title": "INSPIRE XML",
    "section": "Transformation tools & practical tips",
    "text": "Transformation tools & practical tips\n\nHALE (HUMBOLDT Alignment Editor) — widely used to map relational/CSV source schemas to INSPIRE GML; can be automated and reused. See practical examples where SOTER→INSPIRE alignments used HALE. (isric.org)\nScripting/ETL — Python (GDAL/OGR, lxml), FME, or custom tools can generate GML if you prefer code-based pipelines. For complex mappings, HALE + scripts work well.\nDatastore — keep canonical geometries and attributes in PostGIS; publish GML via GeoServer, pygeoapi or an INSPIRE-capable WFS provider."
  },
  {
    "objectID": "use-cases/inspire-xml.html#validation-testing",
    "href": "use-cases/inspire-xml.html#validation-testing",
    "title": "INSPIRE XML",
    "section": "Validation & testing",
    "text": "Validation & testing\n\nUse the INSPIRE Abstract Test Suite and available validation services to check conformance to the Soil data specification and GML application schemas. Capture and publish validation reports with dataset DOI. (GitHub)\nRun XSD validation, schema-aware tests (cardinality, mandatory elements), and domain checks (units, allowed codes)."
  },
  {
    "objectID": "use-cases/inspire-xml.html#metadata-repository-deposit",
    "href": "use-cases/inspire-xml.html#metadata-repository-deposit",
    "title": "INSPIRE XML",
    "section": "Metadata & repository deposit",
    "text": "Metadata & repository deposit\n\nProduce ISO 19115/19139 metadata for the GML dataset and publish this alongside or embedded in the repository record. Also publish a DCAT or Dublin Core landing-page record for the academic repository. (INSPIRE Knowledge Base)\nRepository package should contain: canonical GML dump (.xml/.gml), XSDs used, mapping spec (HALE project or mapping files), validation report(s), a GeoPackage/CSV extract, README and license. Mint a DOI for the package."
  },
  {
    "objectID": "use-cases/index.html",
    "href": "use-cases/index.html",
    "title": "Use cases soil data",
    "section": "",
    "text": "A selection of use cases, combined with dedicated guidance on how to increase FAIR aspect of a data or knowledge publication.\nThe cases are clustered in three topics - Soil observation data - Soil Maps, predicted distribution of soil properties or soil classifications. - Soil data related resources"
  },
  {
    "objectID": "use-cases/index.html#common-aspects-in-describing-observation-data",
    "href": "use-cases/index.html#common-aspects-in-describing-observation-data",
    "title": "Use cases soil data",
    "section": "Common aspects in describing observation data",
    "text": "Common aspects in describing observation data\nWhile SoilWise supports an array of different encodings for observation data, each tailored for different usage areas, these models are united through the underlying conceptual OGC/ISO Observations, Measurements and Samples Standard, semantically formalized under W3C SSN/SOSA. This model entails the use of generic Observation concepts.\nRead more"
  },
  {
    "objectID": "use-cases/index.html#tabular-soil-observation-data",
    "href": "use-cases/index.html#tabular-soil-observation-data",
    "title": "Use cases soil data",
    "section": "Tabular Soil Observation Data",
    "text": "Tabular Soil Observation Data\nTabular Soil Observation Data is a common format to share Soil data.\nThis tutorial considers various aspects on existing practices and suggest minor optimisations and potential to these workflows."
  },
  {
    "objectID": "use-cases/index.html#inspire-soil-data-in-a-geopackage-format",
    "href": "use-cases/index.html#inspire-soil-data-in-a-geopackage-format",
    "title": "Use cases soil data",
    "section": "INSPIRE Soil data in a GeoPackage format",
    "text": "INSPIRE Soil data in a GeoPackage format\nGeoPackage format for sharing INSPIRE SOIL data is an approach to store data in a relational database while conforming to the INSPIRE Soil conceptual model. Traditionally INSPIRE Soil Observation data and maps require to be shared using a dedicated GML encoding, which was quite a challenge for both data procucers and consumers. The INSPIRE Good Practice using GeoPackage specifically for Soil data has been prepared by the EJP Soil H2020 programme and is further developed in the SoilWise project. - Introduction to the INSPIRE SOIL GeoPackage - GeoPackage Data Loading & Modelling Guide - Tutorial on how to use QGIS to populate the GeoPackage"
  },
  {
    "objectID": "use-cases/index.html#sharing-predicted-soil-properties-based-on-spectroscopy-data",
    "href": "use-cases/index.html#sharing-predicted-soil-properties-based-on-spectroscopy-data",
    "title": "Use cases soil data",
    "section": "Sharing predicted soil properties, based on spectroscopy data",
    "text": "Sharing predicted soil properties, based on spectroscopy data\nSpectroscopy is a relatively novel approach to observe soil properties on soil samples efficiently. The properties are predicted based on a library of reference samples which are assessed both with wet chemistry as well as spectroscopy. A common practice is to deposit the spectra of the observations as part of the data as well as a reference to the spectral library used for the prediction."
  },
  {
    "objectID": "use-cases/index.html#earth-imagery",
    "href": "use-cases/index.html#earth-imagery",
    "title": "Use cases soil data",
    "section": "Earth imagery",
    "text": "Earth imagery\nEarth imagery such as satelite and drone data are increasingly used to better understand soils and their interaction with climate, landscape and crops. Imagery data is often used as a predictor of soil property distribution. Due to their size and origin imagery data is usually not deposited in formal repositories, but available from mass storage such as Google Earth Engine, Copernicus or Amazon Cloud, which can be a challenge for citation of the material."
  },
  {
    "objectID": "use-cases/index.html#soil-property-and-indicator-maps",
    "href": "use-cases/index.html#soil-property-and-indicator-maps",
    "title": "Use cases soil data",
    "section": "Soil property and indicator maps",
    "text": "Soil property and indicator maps\nSoil property and indicator maps, such as soil property distribution predictions, agricultural decision support and soil health indicator changes are produced with conventional and pedometric approaches. An important factor to facilitate reuse is extensive description of lineage. Which source data and procedures have been used to derive the resulting product."
  },
  {
    "objectID": "use-cases/index.html#archiving-project-websites-and-knowledge-hubs",
    "href": "use-cases/index.html#archiving-project-websites-and-knowledge-hubs",
    "title": "Use cases soil data",
    "section": "Archiving Project Websites and Knowledge Hubs",
    "text": "Archiving Project Websites and Knowledge Hubs\nProject Websites and Knowledge Hubs benefit from archival of their content at various stages during and after the realisation and maintenance of the website. A deposit of the website content is persistently citable using DOI and the risk of information loss at website discontinuation is limited."
  },
  {
    "objectID": "use-cases/index.html#reuse-of-common-soil-vocabularies",
    "href": "use-cases/index.html#reuse-of-common-soil-vocabularies",
    "title": "Use cases soil data",
    "section": "Reuse of common Soil vocabularies",
    "text": "Reuse of common Soil vocabularies\nSoil vocabularies (glossaries) are commonly produced at the start of projects, to align definitions between project partners. Vocabularies are also an important driver of findability and interoperabilty."
  },
  {
    "objectID": "use-cases/index.html#harmonizing-soil-observation-data-using-pedo-transfer-funtions",
    "href": "use-cases/index.html#harmonizing-soil-observation-data-using-pedo-transfer-funtions",
    "title": "Use cases soil data",
    "section": "Harmonizing Soil Observation data using Pedo Transfer Funtions",
    "text": "Harmonizing Soil Observation data using Pedo Transfer Funtions\nSoil observation datasets are often heterogeneous due to differences in sampling depth, measurement techniques, laboratory methods, temporal coverage, and reported soil properties. Harmonization is the process of transforming these disparate observations into a consistent and comparable form suitable for integrated analysis, modeling, and decision-making. This document describes the current state on this topic."
  },
  {
    "objectID": "use-cases/glosis.html",
    "href": "use-cases/glosis.html",
    "title": "Glosis Web Ontology",
    "section": "",
    "text": "Use the GloSIS ontology network (master module + domain modules) to represent soil profiles, plots, samples, observations, and laboratory procedures in RDF. ([GitHub][1])\nReuse the formal patterns GloSIS relies on: SOSA/SSN for observations, GeoSPARQL for geospatial, QUDT for units/quantities and SKOS for code lists. This increases compatibility with other environmental/observational data. ([arXiv][2])\nProduce: (a) an RDF dataset (Turtle or TriG) derived from your source spreadsheets/CSV/DB, (b) SHACL validation shapes, (c) a machine-readable dataset landing page (DCAT/Dublin Core + citation + DOI), and (d) a SPARQL endpoint / triple store for programmatic reuse and discovery."
  },
  {
    "objectID": "use-cases/glosis.html#identify-and-model-core-entities-map-your-schema-glosis",
    "href": "use-cases/glosis.html#identify-and-model-core-entities-map-your-schema-glosis",
    "title": "Glosis Web Ontology",
    "section": "Identify and model core entities (map your schema → GloSIS)",
    "text": "Identify and model core entities (map your schema → GloSIS)\nMinimum entities you should model:\n\nSite (area investigated) → GloSIS GL_Site / site-plot module. ([w3id.org][3])\nPlot (sampling plot) → GL_Plot. ([w3id.org][3])\nProfile / Horizon → GL_Profile, Layer/Horizon classes (layer-horizon module). ([w3id.org][4])\nSample (physical sample from horizon/plot) → link to sample identifier, date, collector, GPS geometry.\nObservation / Measurement → use SOSA Observation as GloSIS aligns observations with SOSA/OM ontology patterns (e.g., measurement event, result, observedProperty). Use QUDT units for numeric results. ([arXiv][2])\nProcedure / Lab method → GloSIS procedure module for standardized lab codes and method reference lists. ([GitHub][5])\n\nTip: Prefer modeling the observation event (SOSA Observation) that points to a FeatureOfInterest (the sample or horizon) and a Result that carries numeric value + unit."
  },
  {
    "objectID": "use-cases/glosis.html#use-persistent-resolvable-iris",
    "href": "use-cases/glosis.html#use-persistent-resolvable-iris",
    "title": "Glosis Web Ontology",
    "section": "Use persistent, resolvable IRIs",
    "text": "Use persistent, resolvable IRIs\n\nMint IRIs under your domain, e.g., https://data.example.org/soil/site/XXXX, .../sample/YYY, and ensure content negotiation serves HTML landing pages and RDF (Turtle).\nWhere possible, adopt w3id.org or the GloSIS w3id IRIs for ontology terms; GloSIS modules are available under stable w3id IRIs. ([w3id.org][3])"
  },
  {
    "objectID": "use-cases/glosis.html#units-and-code-lists",
    "href": "use-cases/glosis.html#units-and-code-lists",
    "title": "Glosis Web Ontology",
    "section": "Units and code lists",
    "text": "Units and code lists\n\nMap measurement units to QUDT or a controlled URI list (e.g., grams per kg → QUDT URI). This avoids ambiguous strings such as “g/kg” vs “g per kg”. ([arXiv][2])\nLoad GloSIS codelists (they are provided as SKOS concept schemes in the GloSIS modules) and reuse them for categorical fields (texture, classification, colour). ([GitHub][1])"
  },
  {
    "objectID": "use-cases/glosis.html#provenance-citation",
    "href": "use-cases/glosis.html#provenance-citation",
    "title": "Glosis Web Ontology",
    "section": "Provenance & citation",
    "text": "Provenance & citation\n\nUse PROV-O to capture: who ingested data, original source (DB or CSV), transformation script version, and lab certificate/procedure identifiers.\nProvide dataset metadata: title, authors, affiliation, license, DOI, version, contact person — encode as Dublin Core / DCAT on the landing page and in a machine-readable dcat:Dataset RDF description."
  },
  {
    "objectID": "use-cases/glosis.html#validation-shacl",
    "href": "use-cases/glosis.html#validation-shacl",
    "title": "Glosis Web Ontology",
    "section": "Validation (SHACL)",
    "text": "Validation (SHACL)\n\nImplement SHACL shapes for:\n\nrequired properties (site must have geometry, sample must reference a profile/horizon),\nunit constraints (numeric observations must have associated QUDT unit),\nallowed SKOS codes for categorical fields,\ncardinality (e.g., each sample must have exactly one sampling date).\n\nRun SHACL during ETL and as a pre-publication gate."
  },
  {
    "objectID": "use-cases/earth-imagery.html",
    "href": "use-cases/earth-imagery.html",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Earth observation data—such as satellite, airborne, and drone imagery—are increasingly used to model, understand, and predict soil properties, soil health indicators, and landscape interactions. While these datasets are often too large or externally hosted to deposit in traditional research repositories, they must still be FAIR: Findable, Accessible, Interoperable, and Reusable—especially when used as covariates in soil mapping or related earth system products.\nThis guidance explains how to properly reference, document, and ensure traceability of earth imagery in FAIR-compliant publications.\n\n\n\nUnlike traditional datasets:\n\nEarth imagery is high-volume and externally hosted.\nData is typically accessed from platforms or cloud services (e.g. Google Earth Engine, Copernicus, Amazon Web Services, NASA DAACs) rather than institutional repositories.\nFrequently updated versions (e.g. Sentinel, Landsat, PlanetScope) complicate referencing.\nDerived products (e.g. NDVI, texture layers, composites) often lack explicit citation metadata.\n\nDespite these challenges, FAIR publication requires persistent references, transparency of processing, and reproducibility.\n\n\n\n\n\nEven if large imagery files aren’t deposited in repositories:\n\nIdentify data source/platform (e.g. Google Earth Engine, Sentinel Hub, Copernicus DIAS).\nInclude dataset identifiers:\n\nMission/sensor (e.g. Sentinel-2 MSI, Landsat 8 OLI, PlanetScope)\nProduct name and processing level (e.g. L2A surface reflectance)\nTile or scene ID(s), date ranges, and spatial extent\nCloud collection ID or asset ID (if available)\n\nIf a derived layer (e.g. NDVI or PCA stack) was used, document the algorithm and input scenes.\n\n\n\n\nInstead of a repository deposit:\n\nDocument how and where imagery can be accessed.\n\nPlatform or API (e.g. COPERNICUS/S2_SR, LANDSAT/LC08/C02/T1_L2)\nAccess conditions (open data, license requirements, API keys)\nLinks to provider documentation (e.g. Copernicus Hub, USGS Earth Explorer)\n\n\nIf imagery access requires credentials, include:\n\nInstructions for obtaining access.\nReference to the licensing terms.\n\n\n\n\nTo promote reuse and linkage:\n\nRecord CRS (e.g. EPSG:4326 or UTM Zone 33N)\nSpecify file formats (e.g. GeoTIFF, COG, NetCDF, JPEG2000)\nUse machine-readable metadata formats where possible (e.g. STAC, ISO 19115)\n\n\n\n\nEnsure traceability and transparency by documenting:\n\nAcquisition dates or time windows\nPreprocessing steps (e.g. atmospheric correction, mosaicking, scaling)\nDerived layers created (e.g. vegetation index, bare soil index, elevation derivatives)\nVersion or revision number of the source dataset\nCitation or attribution guidelines of the data provider\n\n\n\n\n\nEven when imagery is not physically deposited, it must still be citable using a combination of:\n\nSource dataset name and identifier\nAccess platform/service\nAcquisition time window\nProcessing steps or composite method\nPersistent identifiers where available (e.g. DOI, STAC item ID, AWS S3 path, USGS/ESA scene ID)\n\n\n\n\nAt minimum, store or publish the following for each imagery product used as a covariate:\n\n\n\n\n\n\n\nMetadata Element\nRequirement\n\n\n\n\nData source/platform\ne.g. Copernicus, USGS, Planet, GEE, NASA DAAC\n\n\nSensor/mission\ne.g. Sentinel-2 MSI, Landsat 9 OLI\n\n\nProduct level\ne.g. L1C, L2A, BRDF corrected\n\n\nTemporal coverage\nExact date(s) or period\n\n\nSpatial footprint\nBounding box, region name, or tile ID\n\n\nResolution\nSpatial, temporal, spectral\n\n\nAccess URL or collection ID\nInclude STAC or cloud collection URI if available\n\n\nVersion or processing baseline\nRequired for reproducibility\n\n\nDerived indices/processes\ne.g. NDVI, mosaics, gap-filling, resampling\n\n\nLicense and attribution\ne.g. ESA Open Access License\n\n\nPersistent link (if any)\nDOI, handle, cloud object path\n\n\n\n\n\n\nWhen imagery is used to generate predictors for soil modeling:\n\nDocument computation steps (e.g. bands, indices, thresholds).\nRecord software, libraries, and versions (e.g. GEE script ID, Python/R code).\nProvide links or copies of scripts where possible.\nStore exact datasets and date filters.\n\nIf storing the derived product is infeasible:\n\nShare the workflow and references to input layers instead.\n\n\n\n\nWhen satellite or drone imagery is used as covariates in soil prediction or mapping:\n\nInclude full citation(s) in the metadata of the soil product.\nFor each covariate, mention:\n\nImagery source & version\nDate or period used\nTransformation or resampling method\nResolution and CRS\n\nProvide a data dictionary or table mapping covariate names to imagery sources.\n\n\n\n\nEven though imagery files aren’t uploaded:\n\nPublish metadata and citation details in the FAIR repository alongside the soil product.\nLink imagery inputs via:\n\nDOI placeholders (if available)\nCloud-native identifiers (STAC, S3 path, Earth Engine assets)\nDataset citation strings\n\n\nWhere derived imagery is small enough (e.g. aggregated rasters), include those layers directly.\n\n\n\n\n\n\n\n\n\n\nGoal\nWhat to Do\n\n\n\n\nEnsure Findability\nCite source, collection ID, tile/date info\n\n\nMake Accessible\nProvide access path or platform instructions\n\n\nSupport Interoperability\nUse standard CRS, formats, machine-readable metadata\n\n\nEnable Reuse\nDocument preprocessing, versions, licenses, and citations\n\n\n\n\n\n\nFAIR publication of earth imagery are challenging for full file deposition, but it does require traceability, transparency, and citable identifiers. By documenting access pathways, identifiers, transformations, and licensing, earth imagery remains a reusable component of derived data products."
  },
  {
    "objectID": "use-cases/earth-imagery.html#purpose",
    "href": "use-cases/earth-imagery.html#purpose",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Earth observation data—such as satellite, airborne, and drone imagery—are increasingly used to model, understand, and predict soil properties, soil health indicators, and landscape interactions. While these datasets are often too large or externally hosted to deposit in traditional research repositories, they must still be FAIR: Findable, Accessible, Interoperable, and Reusable—especially when used as covariates in soil mapping or related earth system products.\nThis guidance explains how to properly reference, document, and ensure traceability of earth imagery in FAIR-compliant publications."
  },
  {
    "objectID": "use-cases/earth-imagery.html#key-challenges",
    "href": "use-cases/earth-imagery.html#key-challenges",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Unlike traditional datasets:\n\nEarth imagery is high-volume and externally hosted.\nData is typically accessed from platforms or cloud services (e.g. Google Earth Engine, Copernicus, Amazon Web Services, NASA DAACs) rather than institutional repositories.\nFrequently updated versions (e.g. Sentinel, Landsat, PlanetScope) complicate referencing.\nDerived products (e.g. NDVI, texture layers, composites) often lack explicit citation metadata.\n\nDespite these challenges, FAIR publication requires persistent references, transparency of processing, and reproducibility."
  },
  {
    "objectID": "use-cases/earth-imagery.html#fair-principles-applied-to-earth-imagery",
    "href": "use-cases/earth-imagery.html#fair-principles-applied-to-earth-imagery",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Even if large imagery files aren’t deposited in repositories:\n\nIdentify data source/platform (e.g. Google Earth Engine, Sentinel Hub, Copernicus DIAS).\nInclude dataset identifiers:\n\nMission/sensor (e.g. Sentinel-2 MSI, Landsat 8 OLI, PlanetScope)\nProduct name and processing level (e.g. L2A surface reflectance)\nTile or scene ID(s), date ranges, and spatial extent\nCloud collection ID or asset ID (if available)\n\nIf a derived layer (e.g. NDVI or PCA stack) was used, document the algorithm and input scenes.\n\n\n\n\nInstead of a repository deposit:\n\nDocument how and where imagery can be accessed.\n\nPlatform or API (e.g. COPERNICUS/S2_SR, LANDSAT/LC08/C02/T1_L2)\nAccess conditions (open data, license requirements, API keys)\nLinks to provider documentation (e.g. Copernicus Hub, USGS Earth Explorer)\n\n\nIf imagery access requires credentials, include:\n\nInstructions for obtaining access.\nReference to the licensing terms.\n\n\n\n\nTo promote reuse and linkage:\n\nRecord CRS (e.g. EPSG:4326 or UTM Zone 33N)\nSpecify file formats (e.g. GeoTIFF, COG, NetCDF, JPEG2000)\nUse machine-readable metadata formats where possible (e.g. STAC, ISO 19115)\n\n\n\n\nEnsure traceability and transparency by documenting:\n\nAcquisition dates or time windows\nPreprocessing steps (e.g. atmospheric correction, mosaicking, scaling)\nDerived layers created (e.g. vegetation index, bare soil index, elevation derivatives)\nVersion or revision number of the source dataset\nCitation or attribution guidelines of the data provider"
  },
  {
    "objectID": "use-cases/earth-imagery.html#citation-practices-for-earth-imagery",
    "href": "use-cases/earth-imagery.html#citation-practices-for-earth-imagery",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Even when imagery is not physically deposited, it must still be citable using a combination of:\n\nSource dataset name and identifier\nAccess platform/service\nAcquisition time window\nProcessing steps or composite method\nPersistent identifiers where available (e.g. DOI, STAC item ID, AWS S3 path, USGS/ESA scene ID)"
  },
  {
    "objectID": "use-cases/earth-imagery.html#recommended-metadata-fields",
    "href": "use-cases/earth-imagery.html#recommended-metadata-fields",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "At minimum, store or publish the following for each imagery product used as a covariate:\n\n\n\n\n\n\n\nMetadata Element\nRequirement\n\n\n\n\nData source/platform\ne.g. Copernicus, USGS, Planet, GEE, NASA DAAC\n\n\nSensor/mission\ne.g. Sentinel-2 MSI, Landsat 9 OLI\n\n\nProduct level\ne.g. L1C, L2A, BRDF corrected\n\n\nTemporal coverage\nExact date(s) or period\n\n\nSpatial footprint\nBounding box, region name, or tile ID\n\n\nResolution\nSpatial, temporal, spectral\n\n\nAccess URL or collection ID\nInclude STAC or cloud collection URI if available\n\n\nVersion or processing baseline\nRequired for reproducibility\n\n\nDerived indices/processes\ne.g. NDVI, mosaics, gap-filling, resampling\n\n\nLicense and attribution\ne.g. ESA Open Access License\n\n\nPersistent link (if any)\nDOI, handle, cloud object path"
  },
  {
    "objectID": "use-cases/earth-imagery.html#reproducibility-for-derived-covariates",
    "href": "use-cases/earth-imagery.html#reproducibility-for-derived-covariates",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "When imagery is used to generate predictors for soil modeling:\n\nDocument computation steps (e.g. bands, indices, thresholds).\nRecord software, libraries, and versions (e.g. GEE script ID, Python/R code).\nProvide links or copies of scripts where possible.\nStore exact datasets and date filters.\n\nIf storing the derived product is infeasible:\n\nShare the workflow and references to input layers instead."
  },
  {
    "objectID": "use-cases/earth-imagery.html#integration-in-fair-soil-products",
    "href": "use-cases/earth-imagery.html#integration-in-fair-soil-products",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "When satellite or drone imagery is used as covariates in soil prediction or mapping:\n\nInclude full citation(s) in the metadata of the soil product.\nFor each covariate, mention:\n\nImagery source & version\nDate or period used\nTransformation or resampling method\nResolution and CRS\n\nProvide a data dictionary or table mapping covariate names to imagery sources."
  },
  {
    "objectID": "use-cases/earth-imagery.html#repository-strategy",
    "href": "use-cases/earth-imagery.html#repository-strategy",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Even though imagery files aren’t uploaded:\n\nPublish metadata and citation details in the FAIR repository alongside the soil product.\nLink imagery inputs via:\n\nDOI placeholders (if available)\nCloud-native identifiers (STAC, S3 path, Earth Engine assets)\nDataset citation strings\n\n\nWhere derived imagery is small enough (e.g. aggregated rasters), include those layers directly."
  },
  {
    "objectID": "use-cases/earth-imagery.html#summary-of-best-practices",
    "href": "use-cases/earth-imagery.html#summary-of-best-practices",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "Goal\nWhat to Do\n\n\n\n\nEnsure Findability\nCite source, collection ID, tile/date info\n\n\nMake Accessible\nProvide access path or platform instructions\n\n\nSupport Interoperability\nUse standard CRS, formats, machine-readable metadata\n\n\nEnable Reuse\nDocument preprocessing, versions, licenses, and citations"
  },
  {
    "objectID": "use-cases/earth-imagery.html#conclusion",
    "href": "use-cases/earth-imagery.html#conclusion",
    "title": "Guidance for FAIR Publication and Citation of Earth Imagery in Soil Research",
    "section": "",
    "text": "FAIR publication of earth imagery are challenging for full file deposition, but it does require traceability, transparency, and citable identifiers. By documenting access pathways, identifiers, transformations, and licensing, earth imagery remains a reusable component of derived data products."
  },
  {
    "objectID": "General_Guidelines.html",
    "href": "General_Guidelines.html",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "",
    "text": "WarningDraft Version\n\n\n\nThis document is a work in progress and not the final version.\nSoil-related data and knowledge are essential for understanding and improving soil health, guiding sustainable land management, informing policy, and enabling scientific progress. Yet, these resources are often scattered, poorly documented, hard to access, or locked in incompatible formats. This fragmentation hampers their reuse, reduces their impact, and creates inefficiencies across projects and disciplines.\nThis strategy, developed within the SoilWise project, provides guidance to ensure that all data and knowledge generated, collected, or reused within Mission Soil projects are managed according to the FAIR principles – Findable, Accessible, Interoperable, and Reusable.\nBy applying the FAIR principles, we can increase the visibility and value of soil-related data and knowledge. FAIRness enables:\nIt is intended not only as a conceptual framework, but also as a practical tool to support project teams in developing their own Data Management Plans (DMPs). Each project or work package should describe how its data and knowledge will be handled, shared, and sustained in a FAIR-compliant way.\nSoil health knowledge is inherently diverse – ranging from field measurements and lab results to local experiences and spatial models. This strategy promotes coherence and openness while respecting contextual and disciplinary differences.\nMetadata play a central role in this process: without clear and rich metadata, data cannot be truly findable or reusable. Metadata describe the context, origin, structure, and conditions of use of the data or knowledge asset, acting as the bridge between producers and users — whether human or machine."
  },
  {
    "objectID": "General_Guidelines.html#approach-how-to-fairify-data-and-knowledge",
    "href": "General_Guidelines.html#approach-how-to-fairify-data-and-knowledge",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Approach: How to FAIRify Data and Knowledge",
    "text": "Approach: How to FAIRify Data and Knowledge\nWe recommend the following stepwise process for implementing FAIR principles during metadata population."
  },
  {
    "objectID": "General_Guidelines.html#step-1-inventory-typology",
    "href": "General_Guidelines.html#step-1-inventory-typology",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 1: Inventory & Typology",
    "text": "Step 1: Inventory & Typology\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Identify what data and knowledge will be created or reused.  - Classify them by type, format, origin, and reuse potential.\n- During project planning or Data Management Plan (DMP) preparation, make a list of all expected outputs: datasets, reports, maps, models, code, workflows, protocols, websites, and other knowledge products. - For each resource, note whether it is newly created, collected from partners, or reused from external sources. - Record the data type and format (e.g., raster, tabular, text, time series)."
  },
  {
    "objectID": "General_Guidelines.html#step-2-choose-trustworthy-repositories",
    "href": "General_Guidelines.html#step-2-choose-trustworthy-repositories",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 2: Choose Trustworthy Repositories",
    "text": "Step 2: Choose Trustworthy Repositories\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Use trustworthy platforms for archiving and sharing  - Prefer repositories that support persistent identifiers (e.g., DOI), machine-readable metadata, and open licenses. Remember that FAIR doesn’t necessarily mean open. In most repositories, you can also safely store resources with restricted access.\n- Choose a repository before your project outputs are finalized, so you can prepare metadata accordingly.  - For academic outputs, use Zenodo (linked to OpenAIRE). For other contexts, explore re3data.org to identify certified or domain-specific repositories.  - Verify that the repository supports persistent identifiers (PID, DOI) and offers clear license options (e.g., Creative Commons).  - Check repository policy: does it allow embargo periods or restricted access for sensitive data?\n\n\n\nHow to be discoverable by Soil Wise harvesting strategies? For outputs of Horizon Europe research, verify that the selected repository is harvested by OpenAire and that the Grant number of the project can be added as part of the funding information. SoilWise and Cordis use this information to select relevant publications. For non-academic repositories, verify that the repository is harvested by data.europa.eu and add a keyword soil to the metadata record (in the relevant language of the record). Notice that data.europa.eu includes the contents of ESDAC as well as INSPIRE Geoportal."
  },
  {
    "objectID": "General_Guidelines.html#step-3-choose-open-and-standardized-formats",
    "href": "General_Guidelines.html#step-3-choose-open-and-standardized-formats",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 3: Choose Open and Standardized Formats",
    "text": "Step 3: Choose Open and Standardized Formats\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Use non-proprietary, open formats whenever possible.  - Ensure that data formats are machine-readable and suitable for long-term access.\n- If possible, when preparing data for sharing, export or convert them into open formats:  - CSV, TSV → for tabular data - GeoJSON, GML → for vector spatial data - GeoTIFF → for raster data - PDF/A or TXT → for documents - SQLite, PostgreSQL dumps → for databases - Avoid file formats that depend on proprietary software (e.g., XLSX, MDB, DOCX). - If necessary for reusability or additional description of your data, include a README.txt file in the dataset or archive describing: - data structure, variable descriptions, units - coordinate system (for spatial data) - software used to create or open files - known limitations or assumptions.  - Compress related files into a ZIP archive before uploading, ensuring all dependencies are included."
  },
  {
    "objectID": "General_Guidelines.html#step-4-metadata-design-and-management",
    "href": "General_Guidelines.html#step-4-metadata-design-and-management",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 4: Metadata Design and Management",
    "text": "Step 4: Metadata Design and Management\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Populate as much metadata as you can and describe all the information needed to understand and use your data. You will support the Reusability of the resource.  - Assign persistent identifiers (PIDs) to data objects, documents, and authors whenever possible.  - Use metadata to interlink related resources.\n- Create metadata at the time of upload or using metadata templates provided by your repository.  - Fill in all key fields: title, creators (with ORCID if possible), description, keywords, publication date, funding, related resources, license, and version.  - Use clear, descriptive titles and structured abstracts that explain what the dataset contains, how it was produced, and its spatial/temporal coverage.  - Assign persistent identifiers (PIDs): - Dataset → DOI - People → ORCID - Projects → Grant number / ROR (Research Organization Registry  - Interlink related resources, such as datasets, publications, methodologies, software or reports. Most repositories allow “Related identifiers”.  - Keep a local copy of metadata (e.g., in JSON, XML, or CSV format) as a backup for institutional archiving."
  },
  {
    "objectID": "General_Guidelines.html#step-5-use-controlled-vocabularies-and-ontologies",
    "href": "General_Guidelines.html#step-5-use-controlled-vocabularies-and-ontologies",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 5: Use Controlled Vocabularies and Ontologies",
    "text": "Step 5: Use Controlled Vocabularies and Ontologies\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Apply standardized vocabularies, code lists or established domain terms to describe data content and context.  - If possible, avoid creating new vocabularies and ontologies in your project. When local or project-specific terms are used, provide crosswalks or mappings to widely used vocabularies.\n- Identify the relevant vocabulary or established domain terms source early in the project:  - AGROVOC – for agricultural concepts  - GEMET – for environmental terms  - INSPIRE themes – for spatial data  - WRB soil classification  - LUCAS codes  - EUNIS habitat codes- Use the suggested keywords from your repository’s predefined list — this ensures linking to standard vocabularies automatically. - For attributes or variables in datasets, provide code lists or lookup tables that explain categorical values (e.g., soil texture codes). - When using project-specific terminology, create a mapping table linking local terms to standard equivalents (crosswalk). Store it with your dataset.- For ontologies or semantic models, reference them by URI where possible (e.g., from AgroPortal or BioPortal)."
  },
  {
    "objectID": "General_Guidelines.html#step-6-define-licensing-and-access-conditions",
    "href": "General_Guidelines.html#step-6-define-licensing-and-access-conditions",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 6: Define Licensing and Access Conditions",
    "text": "Step 6: Define Licensing and Access Conditions\n\n\n\nPrinciples\nHow to\n\n\n\n\n- Clearly specify the terms under which data and knowledge can be reused. Always use already existing, standardised licences.  - Provide machine-readable licensing information in metadata.  - FAIR doesn’t necessarily mean open. In most repositories, you can also safely store resources with restricted access.\n- Before publication, review whether your data contains personal, confidential, or sensitive information (e.g., farmer names, coordinates of private land).  - If necessary, anonymize or aggregate data before release. - Choose a standard license compatible with your data type (If possible, choose one of the Creative Commons licenses. They are widely used, and easy to understand), such as: - CC-BY 4.0 – requires attribution (recommended default)- CC0 – public domain- CC-BY-NC – non-commercial use- ODC-BY – for databases - Include the license text or identifier (e.g., https://creativecommons.org/licenses/by/4.0/) in your metadata record. - Choose the most open license possible, while respecting privacy, ethical and legal constraints. - Ensure the repository captures the license as machine-readable metadata. - If access restrictions apply, specify the access level (open, embargoed, restricted) and provide contact information for requests. - Mention licensing and access terms clearly in your DMP and README file. - When in doubt, consult your institutional data steward or legal office before choosing the license.\n\n\n\n\nIf you are part of a Mission Soil project and want your resources to be findable through the SoilWise Catalogue, the easiest way is to upload them to Zenodo and link the assets to your project (in the “Funding” section). It will be automatically harvested and indexed in the SoilWise Catalogue within a few days.\n\nFor more information on FAIR principles, you can visit guides at e.g. OpenAIRE or GO FAIR, or you can test your resource FAIRness at FAIR Data Self-Assessment or FAIRsFAIR tools. See also the related official Horizon 2020 documents, Guidelines on FAIR Data Management in Horizon 2020, and Guidelines to the Rules on Open Access to Scientific Publications and Open Access to Research Data in Horizon 2020."
  },
  {
    "objectID": "General_Guidelines_v0.2.html",
    "href": "General_Guidelines_v0.2.html",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "",
    "text": "WarningDraft Version\n\n\n\nThis document is a work in progress and not the final version.\nSoil-related data and knowledge are essential for understanding and improving soil health, guiding sustainable land management, informing policy, and enabling scientific progress. Yet, these resources are often scattered, poorly documented, hard to access, or locked in incompatible formats. This fragmentation hampers their reuse, reduces their impact, and creates inefficiencies across projects and disciplines.\nThis strategy, developed within the SoilWise project, provides guidance to ensure that all data and knowledge generated, collected, or reused within Mission Soil projects are managed according to the FAIR principles – Findable, Accessible, Interoperable, and Reusable.\nBy applying the FAIR principles, we can increase the visibility and value of soil-related data and knowledge. FAIRness enables:\nIt is intended not only as a conceptual framework, but also as a practical tool to support project teams in developing their own Data Management Plans (DMPs). Each project or work package should describe how its data and knowledge will be handled, shared, and sustained in a FAIR-compliant way.\nSoil health knowledge is inherently diverse – ranging from field measurements and lab results to local experiences and spatial models. This strategy promotes coherence and openness while respecting contextual and disciplinary differences.\nMetadata play a central role in this process: without clear and rich metadata, data cannot be truly findable or reusable. Metadata describe the context, origin, structure, and conditions of use of the data or knowledge asset, acting as the bridge between producers and users — whether human or machine."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#approach-how-to-fairify-data-and-knowledge",
    "href": "General_Guidelines_v0.2.html#approach-how-to-fairify-data-and-knowledge",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Approach: How to FAIRify Data and Knowledge",
    "text": "Approach: How to FAIRify Data and Knowledge\nWe recommend the following stepwise process for implementing FAIR principles during metadata population."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-1-inventory-typology",
    "href": "General_Guidelines_v0.2.html#step-1-inventory-typology",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 1: Inventory & Typology",
    "text": "Step 1: Inventory & Typology\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Identify what data and knowledge will be created or reused.  • Classify them by type, format, origin, and reuse potential.\n• During project planning or Data Management Plan (DMP) preparation, make a list of all expected outputs: datasets, reports, maps, models, code, workflows, protocols, websites, and other knowledge products. • For each resource, note whether it is newly created, collected from partners, or reused from external sources. • Record the data type and format (e.g., raster, tabular, text, time series)."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-2-choose-trustworthy-repositories",
    "href": "General_Guidelines_v0.2.html#step-2-choose-trustworthy-repositories",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 2: Choose Trustworthy Repositories",
    "text": "Step 2: Choose Trustworthy Repositories\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Use trustworthy platforms for archiving and sharing  • Prefer repositories that support persistent identifiers (e.g., DOI), machine-readable metadata, and open licenses. Remember that FAIR doesn’t necessarily mean open. In most repositories, you can also safely store resources with restricted access.\n• Choose a repository before your project outputs are finalized, so you can prepare metadata accordingly.  • For academic outputs, use Zenodo (linked to OpenAIRE). For other contexts, explore re3data.org to identify certified or domain-specific repositories.  • Verify that the repository supports persistent identifiers (PID, DOI) and offers clear license options (e.g., Creative Commons).  • Check repository policy: does it allow embargo periods or restricted access for sensitive data?\n\n\n\nHow to be discoverable by Soil Wise harvesting strategies? For outputs of Horizon Europe research, verify that the selected repository is harvested by OpenAire and that the Grant number of the project can be added as part of the funding information. SoilWise and Cordis use this information to select relevant publications. For non-academic repositories, verify that the repository is harvested by data.europa.eu and add a keyword soil to the metadata record (in the relevant language of the record). Notice that data.europa.eu includes the contents of ESDAC as well as INSPIRE Geoportal."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-3-choose-open-and-standardized-formats",
    "href": "General_Guidelines_v0.2.html#step-3-choose-open-and-standardized-formats",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 3: Choose Open and Standardized Formats",
    "text": "Step 3: Choose Open and Standardized Formats\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Use non-proprietary, open formats whenever possible.  • Ensure that data formats are machine-readable and suitable for long-term access.\n• If possible, when preparing data for sharing, export or convert them into open formats:  - CSV, TSV → for tabular data - GeoJSON, GML → for vector spatial data - GeoTIFF → for raster data - PDF/A or TXT → for documents - SQLite, PostgreSQL dumps → for databases • Avoid file formats that depend on proprietary software (e.g., XLSX, MDB, DOCX). • If necessary for reusability or additional description of your data, include a README.txt file in the dataset or archive describing: - data structure, variable descriptions, units - coordinate system (for spatial data) - software used to create or open files - known limitations or assumptions.  • Compress related files into a ZIP archive before uploading, ensuring all dependencies are included."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-4-metadata-design-and-management",
    "href": "General_Guidelines_v0.2.html#step-4-metadata-design-and-management",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 4: Metadata Design and Management",
    "text": "Step 4: Metadata Design and Management\n\n\n\n\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Populate as much metadata as you can and describe all the information needed to understand and use your data. You will support the Reusability of the resource.  • Assign persistent identifiers (PIDs) to data objects, documents, and authors whenever possible.  • Use metadata to interlink related resources.\n• Create metadata at the time of upload or using metadata templates provided by your repository.  • Fill in all key fields: title, creators (with ORCID if possible), description, keywords, publication date, funding, related resources, license, and version.  • Use clear, descriptive titles and structured abstracts that explain what the dataset contains, how it was produced, and its spatial/temporal coverage.  • Assign persistent identifiers (PIDs): - Dataset → DOI - People → ORCID - Projects → Grant number / ROR (Research Organization Registry  • Interlink related resources, such as datasets, publications, methodologies, software or reports. Most repositories allow “Related identifiers”.  • Keep a local copy of metadata (e.g., in JSON, XML, or CSV format) as a backup for institutional archiving."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-5-use-controlled-vocabularies-and-ontologies",
    "href": "General_Guidelines_v0.2.html#step-5-use-controlled-vocabularies-and-ontologies",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 5: Use Controlled Vocabularies and Ontologies",
    "text": "Step 5: Use Controlled Vocabularies and Ontologies\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Apply standardized vocabularies, code lists or established domain terms to describe data content and context.  • If possible, avoid creating new vocabularies and ontologies in your project. When local or project-specific terms are used, provide crosswalks or mappings to widely used vocabularies.\n• Identify the relevant vocabulary or established domain terms source early in the project:  - AGROVOC – for agricultural concepts  - GEMET – for environmental terms  - INSPIRE themes – for spatial data  - WRB soil classification  - LUCAS codes  - EUNIS habitat codes• Use the suggested keywords from your repository’s predefined list — this ensures linking to standard vocabularies automatically. • For attributes or variables in datasets, provide code lists or lookup tables that explain categorical values (e.g., soil texture codes). • When using project-specific terminology, create a mapping table linking local terms to standard equivalents (crosswalk). Store it with your dataset.• For ontologies or semantic models, reference them by URI where possible (e.g., from AgroPortal or BioPortal)."
  },
  {
    "objectID": "General_Guidelines_v0.2.html#step-6-define-licensing-and-access-conditions",
    "href": "General_Guidelines_v0.2.html#step-6-define-licensing-and-access-conditions",
    "title": "6 Steps Towards FAIR Data and Knowledge in Mission Soil",
    "section": "Step 6: Define Licensing and Access Conditions",
    "text": "Step 6: Define Licensing and Access Conditions\n\n\n\nPrinciples\nHow to\n\n\n\n\n• Clearly specify the terms under which data and knowledge can be reused. Always use already existing, standardised licences.  • Provide machine-readable licensing information in metadata.  • FAIR doesn’t necessarily mean open. In most repositories, you can also safely store resources with restricted access.\n• Before publication, review whether your data contains personal, confidential, or sensitive information (e.g., farmer names, coordinates of private land).  • If necessary, anonymize or aggregate data before release. • Choose a standard license compatible with your data type (If possible, choose one of the Creative Commons licenses. They are widely used, and easy to understand), such as: - CC-BY 4.0 – requires attribution (recommended default)- CC0 – public domain- CC-BY-NC – non-commercial use- ODC-BY – for databases • Include the license text or identifier (e.g., https://creativecommons.org/licenses/by/4.0/) in your metadata record. • Choose the most open license possible, while respecting privacy, ethical and legal constraints. • Ensure the repository captures the license as machine-readable metadata. • If access restrictions apply, specify the access level (open, embargoed, restricted) and provide contact information for requests. • Mention licensing and access terms clearly in your DMP and README file. • When in doubt, consult your institutional data steward or legal office before choosing the license.\n\n\n\n\nIf you are part of a Mission Soil project and want your resources to be findable through the SoilWise Catalogue, the easiest way is to upload them to Zenodo and link the assets to your project (in the “Funding” section). It will be automatically harvested and indexed in the SoilWise Catalogue within a few days.\n\nFor more information on FAIR principles, you can visit guides at e.g. OpenAIRE or GO FAIR, or you can test your resource FAIRness at FAIR Data Self-Assessment or FAIRsFAIR tools. See also the related official Horizon 2020 documents, Guidelines on FAIR Data Management in Horizon 2020, and Guidelines to the Rules on Open Access to Scientific Publications and Open Access to Research Data in Horizon 2020."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html",
    "href": "use-cases/gbif-biodiversity-data.html",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Platforms like GBIF can strongly support FAIR data. But only if data providers set things up properly and users reference their data downloads properly. This documents presents a strategy to help you make your biodiversity data as Findable, Accessible, Interoperable, and Reusable (FAIR) as possible.\nBy design, datasets in GBIF are incremental, new biodiversity observations can arrive at a daily basis. An identifier is assigned to incremental datasets, and the datasets are findable through OpenAire. However for users to reference the state of the dataset, when using it in their research. they can make an explicit download of the dataset. GBIF will create a persistent identifiation to that snapshot, which can be used as a reference. There is no need to deposit the extracted dataset separately.\n\n\n\n\n\nMaintain your source data in a version-controlled or otherwise documented system.\nKeep track of changes so you can update GBIF intentionally and transparently.\n\n\n\n\n\nStructure data clearly using DwC terms (e.g., occurrenceID, eventDate, scientificName).\nAvoid custom field names unless necessary—and document them.\n\n\n\n\n\nUse a standard open license like CC0 or CC BY 4.0, as required by GBIF.\nClearly define attribution expectations in your metadata.\n\n\n\n\n\nUse stable, provider-managed occurrenceIDs, not row numbers or UUIDs that change.\nAvoid reassigning IDs across updates.\n\n\n\n\n\n\n\n\nEnsure the source repository (IPT, DiGIR, BioCASE, etc.) is up-to-date and maintained.\nUse HTTPS endpoints where possible.\n\n\n\n\nInclude:\n\nDataset purpose and scope\nGeographic and taxonomic coverage\nMethods, sampling protocols, and data quality notes\nContact details and institutional information\nUpdate frequency and version history\n\n\n\n\n\nGBIF assigns a concept DOI for the dataset.\nConfirm the DOI resolves correctly and includes citation guidance.\nIf you publish major revisions, consider creating a new versioned DOI.\n\n\n\n\n\n\n\n\nIf you update content (records, taxonomy, or coverage), revise the metadata too.\nAnnotate major updates with version numbers or changelogs.\n\n\n\n\nGBIF may flag:\n\nDuplicate IDs\nInvalid coordinates\nTaxonomic mismatches\nMissing fields\n\nRespond and correct issues promptly to preserve usability.\n\n\n\nEncourage users to:\n\nCite download DOIs for snapshots (not just the dataset DOI).\nReach out if clarifications are needed.\n\n\n\n\n\n\n\n\nTaxonomy: match to recognised checklists whenever appropriate.\nLocations: use standardized coordinate systems and georeferencing protocols.\nDates: follow ISO 8601 (e.g., YYYY-MM-DD).\n\n\n\n\nExample additions:\n\nSampling methods (samplingProtocol)\nBasis of record (materialSample, humanObservation, etc.)\nLife stage, sex, habitat\nCitations or related literature\n\n\n\n\nWhen possible, connect:\n\nSpecimen records → collection codes (GRBio, CETAF IDs)\nTaxon concepts → WoRMS, Catalogue of Life\nProjects → institutional repositories or scientific publications\n\n\n\n\n\n\n\n\nProvide a preferred citation format in the metadata.\nEncourage users to use download DOIs for stability.\n\n\n\n\nState whether data will be:\n\nStatic (no updates)\nPeriodically updated\nContinuously synchronized\n\n\n\n\n\nUse this quick checklist:\n\n\n\n\n\n\n\nFAIR Principle\nYou’re doing well if…\n\n\n\n\nFindable\nThe dataset has a DOI, clear title, and searchable metadata.\n\n\nAccessible\nData can be downloaded without barriers and has an open license.\n\n\nInteroperable\nDarwin Core terms and standard vocabularies are used consistently.\n\n\nReusable\nMetadata explains methods, identifiers are stable, and updates are traceable.\n\n\n\n\n\n\nGBIF provides the infrastructure for FAIR data, but providers control the quality and longevity. By supplying structured data, rich metadata, stable identifiers, and clear versioning, you ensure users can reliably find, cite, and reuse your dataset far into the future."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#before-publishing-prepare-your-dataset",
    "href": "use-cases/gbif-biodiversity-data.html#before-publishing-prepare-your-dataset",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Maintain your source data in a version-controlled or otherwise documented system.\nKeep track of changes so you can update GBIF intentionally and transparently.\n\n\n\n\n\nStructure data clearly using DwC terms (e.g., occurrenceID, eventDate, scientificName).\nAvoid custom field names unless necessary—and document them.\n\n\n\n\n\nUse a standard open license like CC0 or CC BY 4.0, as required by GBIF.\nClearly define attribution expectations in your metadata.\n\n\n\n\n\nUse stable, provider-managed occurrenceIDs, not row numbers or UUIDs that change.\nAvoid reassigning IDs across updates."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#publishing-to-gbif-do-it-the-fair-way",
    "href": "use-cases/gbif-biodiversity-data.html#publishing-to-gbif-do-it-the-fair-way",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Ensure the source repository (IPT, DiGIR, BioCASE, etc.) is up-to-date and maintained.\nUse HTTPS endpoints where possible.\n\n\n\n\nInclude:\n\nDataset purpose and scope\nGeographic and taxonomic coverage\nMethods, sampling protocols, and data quality notes\nContact details and institutional information\nUpdate frequency and version history\n\n\n\n\n\nGBIF assigns a concept DOI for the dataset.\nConfirm the DOI resolves correctly and includes citation guidance.\nIf you publish major revisions, consider creating a new versioned DOI."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#after-publishing-maintain-fairness-over-time",
    "href": "use-cases/gbif-biodiversity-data.html#after-publishing-maintain-fairness-over-time",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "If you update content (records, taxonomy, or coverage), revise the metadata too.\nAnnotate major updates with version numbers or changelogs.\n\n\n\n\nGBIF may flag:\n\nDuplicate IDs\nInvalid coordinates\nTaxonomic mismatches\nMissing fields\n\nRespond and correct issues promptly to preserve usability.\n\n\n\nEncourage users to:\n\nCite download DOIs for snapshots (not just the dataset DOI).\nReach out if clarifications are needed."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#enhancing-reuse-and-interoperability",
    "href": "use-cases/gbif-biodiversity-data.html#enhancing-reuse-and-interoperability",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Taxonomy: match to recognised checklists whenever appropriate.\nLocations: use standardized coordinate systems and georeferencing protocols.\nDates: follow ISO 8601 (e.g., YYYY-MM-DD).\n\n\n\n\nExample additions:\n\nSampling methods (samplingProtocol)\nBasis of record (materialSample, humanObservation, etc.)\nLife stage, sex, habitat\nCitations or related literature\n\n\n\n\nWhen possible, connect:\n\nSpecimen records → collection codes (GRBio, CETAF IDs)\nTaxon concepts → WoRMS, Catalogue of Life\nProjects → institutional repositories or scientific publications"
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#communicate-with-gbif-as-a-partner",
    "href": "use-cases/gbif-biodiversity-data.html#communicate-with-gbif-as-a-partner",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Provide a preferred citation format in the metadata.\nEncourage users to use download DOIs for stability.\n\n\n\n\nState whether data will be:\n\nStatic (no updates)\nPeriodically updated\nContinuously synchronized"
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#evaluate-your-fairness",
    "href": "use-cases/gbif-biodiversity-data.html#evaluate-your-fairness",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "Use this quick checklist:\n\n\n\n\n\n\n\nFAIR Principle\nYou’re doing well if…\n\n\n\n\nFindable\nThe dataset has a DOI, clear title, and searchable metadata.\n\n\nAccessible\nData can be downloaded without barriers and has an open license.\n\n\nInteroperable\nDarwin Core terms and standard vocabularies are used consistently.\n\n\nReusable\nMetadata explains methods, identifiers are stable, and updates are traceable."
  },
  {
    "objectID": "use-cases/gbif-biodiversity-data.html#final-takeaway",
    "href": "use-cases/gbif-biodiversity-data.html#final-takeaway",
    "title": "Strategy for ensuring FAIR biodiversiy data via GBIF",
    "section": "",
    "text": "GBIF provides the infrastructure for FAIR data, but providers control the quality and longevity. By supplying structured data, rich metadata, stable identifiers, and clear versioning, you ensure users can reliably find, cite, and reuse your dataset far into the future."
  },
  {
    "objectID": "use-cases/harmonise-ptf.html",
    "href": "use-cases/harmonise-ptf.html",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Soil observation datasets are often heterogeneous due to differences in sampling depth, measurement techniques, laboratory methods, temporal coverage, and reported soil properties. Harmonization is the process of transforming these disparate observations into a consistent and comparable form suitable for integrated analysis, modeling, and decision-making.\nPedotransfer Functions (PTFs) are empirical or semi-empirical relationships that estimate difficult-to-measure soil properties (e.g., hydraulic conductivity, water retention parameters) from more readily available attributes (e.g., texture, bulk density, organic carbon). When applied carefully, PTFs are a powerful tool for harmonizing soil datasets across spatial scales and data sources.\nThis document outlines:\n\nPreconditions for harmonization using PTFs\nRecommended workflows\nUse of existing PTF libraries\nStatistical validation of harmonized outputs\n\n\n\n\n\n\nBefore applying PTFs, ensure the following minimum conditions are met:\n\nMetadata completeness\n\nIdentification of the observed soil property\nSampling depth and method\nLaboratory analysis protocols\nUnit of measurement\n\nSpatial and temporal context\n\nGeographic coordinates (with uncertainty if possible)\nSampling date or period\n\n\n\n\n\nAll input datasets should be standardized prior to PTF application:\n\nUnits harmonization (e.g., % vs g kg⁻¹, cm vs mm)\nDepth normalization (e.g., fixed layers such as 0–5, 5–15, 15–30 cm)\nTextural class consistency (e.g., USDA vs FAO systems)\nOutlier screening using robust statistics or expert thresholds\n\n\n\n\n\n\n\nPTFs can be used to:\n\nFill gaps in missing soil properties\nTranslate measurements between methods\nProduce standardized soil hydraulic or physical parameters\nEnable cross-dataset comparability\n\n\n\n\n\nClass PTFs Based on soil textural or taxonomic classes\nContinuous PTFs Regression-based functions using continuous predictors\nMachine-learning PTFs Neural networks, random forests, or ensemble approaches trained on large soil databases\n\nEach type implies different uncertainty characteristics and validation requirements.\n\n\n\n\n\n\nClearly specify which soil variables are required for harmonization (e.g., field capacity, wilting point, saturated hydraulic conductivity).\n\n\n\nSelection criteria should include:\n\nCompatibility with available predictors\nGeographic or climatic relevance\nTransparency and documentation\nPeer-reviewed validation history\n\n\n\n\n\nUse identical input preprocessing across datasets\nApply depth-specific PTFs where applicable\nRecord model versions and parameter settings\n\n\n\n\nWhere possible:\n\nUse PTFs that provide prediction intervals\nApply Monte Carlo simulations with input uncertainty ranges\nStore uncertainty metrics alongside predictions\n\n\n\n\n\nSeveral well-established libraries and tools are commonly used for soil data harmonization:\n\nROSETTA Estimates soil hydraulic parameters from texture, bulk density, and water retention data.\nHYPRES PTFs Developed for European soils, particularly suited for large-scale modeling.\nSoilGrids-derived PTF frameworks Combine global soil predictions with machine-learning-based PTFs.\nR and Python ecosystems\n\nR packages: soiltexture, hydroGOF, caret\nPython libraries: scikit-learn, pandas, numpy\n\n\nWhen using external libraries:\n\nVerify the training dataset and domain of applicability\nAvoid extrapolation beyond original soil conditions\nDocument assumptions explicitly\n\n\n\n\nValidation is essential to ensure harmonized data are fit for purpose.\n\n\n\nInternal validation\n\nCross-validation within the source dataset\nBootstrapping to assess model stability\n\nExternal validation\n\nIndependent soil datasets\nBenchmarking against measured values\n\n\n\n\n\nCommonly used statistical indicators include:\n\nBias and Mean Error (ME)\nRoot Mean Square Error (RMSE)\nMean Absolute Error (MAE)\nCoefficient of Determination (R²)\nNash–Sutcliffe Efficiency (NSE)\n\nMetrics should be reported by soil depth and, where possible, by soil class.\n\n\n\n\nResidual plots vs predictors and depth\nSensitivity analysis of input variables\nSpatial autocorrelation of errors\n\n\n\n\n\nFor transparency and reuse:\n\nRecord PTF names, equations, or model identifiers\nArchive preprocessing scripts and configuration files\nStore harmonized outputs with uncertainty metadata\nUse version control and persistent identifiers where possible\n\n\n\n\n\nPTFs are context-dependent and not universal\nHarmonization reduces heterogeneity but does not eliminate uncertainty\nAvoid mixing PTF outputs with direct measurements without clear labeling\nAlways match PTF complexity to data quality and project scale\n\n\n\n\nHarmonizing soil observation data using pedotransfer functions is a powerful, but non-trivial, process. Success depends on careful preprocessing, informed PTF selection, rigorous validation, and transparent documentation. When applied responsibly, PTF-based harmonization enables integrated soil analyses across regions, scales, and data sources while preserving scientific credibility."
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#introduction",
    "href": "use-cases/harmonise-ptf.html#introduction",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Soil observation datasets are often heterogeneous due to differences in sampling depth, measurement techniques, laboratory methods, temporal coverage, and reported soil properties. Harmonization is the process of transforming these disparate observations into a consistent and comparable form suitable for integrated analysis, modeling, and decision-making.\nPedotransfer Functions (PTFs) are empirical or semi-empirical relationships that estimate difficult-to-measure soil properties (e.g., hydraulic conductivity, water retention parameters) from more readily available attributes (e.g., texture, bulk density, organic carbon). When applied carefully, PTFs are a powerful tool for harmonizing soil datasets across spatial scales and data sources.\nThis document outlines:\n\nPreconditions for harmonization using PTFs\nRecommended workflows\nUse of existing PTF libraries\nStatistical validation of harmonized outputs"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#prerequisites-for-harmonization",
    "href": "use-cases/harmonise-ptf.html#prerequisites-for-harmonization",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Before applying PTFs, ensure the following minimum conditions are met:\n\nMetadata completeness\n\nIdentification of the observed soil property\nSampling depth and method\nLaboratory analysis protocols\nUnit of measurement\n\nSpatial and temporal context\n\nGeographic coordinates (with uncertainty if possible)\nSampling date or period\n\n\n\n\n\nAll input datasets should be standardized prior to PTF application:\n\nUnits harmonization (e.g., % vs g kg⁻¹, cm vs mm)\nDepth normalization (e.g., fixed layers such as 0–5, 5–15, 15–30 cm)\nTextural class consistency (e.g., USDA vs FAO systems)\nOutlier screening using robust statistics or expert thresholds"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#conceptual-framework-for-using-pedotransfer-functions",
    "href": "use-cases/harmonise-ptf.html#conceptual-framework-for-using-pedotransfer-functions",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "PTFs can be used to:\n\nFill gaps in missing soil properties\nTranslate measurements between methods\nProduce standardized soil hydraulic or physical parameters\nEnable cross-dataset comparability\n\n\n\n\n\nClass PTFs Based on soil textural or taxonomic classes\nContinuous PTFs Regression-based functions using continuous predictors\nMachine-learning PTFs Neural networks, random forests, or ensemble approaches trained on large soil databases\n\nEach type implies different uncertainty characteristics and validation requirements."
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#workflow-for-harmonization-using-ptfs",
    "href": "use-cases/harmonise-ptf.html#workflow-for-harmonization-using-ptfs",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Clearly specify which soil variables are required for harmonization (e.g., field capacity, wilting point, saturated hydraulic conductivity).\n\n\n\nSelection criteria should include:\n\nCompatibility with available predictors\nGeographic or climatic relevance\nTransparency and documentation\nPeer-reviewed validation history\n\n\n\n\n\nUse identical input preprocessing across datasets\nApply depth-specific PTFs where applicable\nRecord model versions and parameter settings\n\n\n\n\nWhere possible:\n\nUse PTFs that provide prediction intervals\nApply Monte Carlo simulations with input uncertainty ranges\nStore uncertainty metrics alongside predictions"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#using-known-libraries-of-pedotransfer-functions",
    "href": "use-cases/harmonise-ptf.html#using-known-libraries-of-pedotransfer-functions",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Several well-established libraries and tools are commonly used for soil data harmonization:\n\nROSETTA Estimates soil hydraulic parameters from texture, bulk density, and water retention data.\nHYPRES PTFs Developed for European soils, particularly suited for large-scale modeling.\nSoilGrids-derived PTF frameworks Combine global soil predictions with machine-learning-based PTFs.\nR and Python ecosystems\n\nR packages: soiltexture, hydroGOF, caret\nPython libraries: scikit-learn, pandas, numpy\n\n\nWhen using external libraries:\n\nVerify the training dataset and domain of applicability\nAvoid extrapolation beyond original soil conditions\nDocument assumptions explicitly"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#statistical-validation-of-harmonized-results",
    "href": "use-cases/harmonise-ptf.html#statistical-validation-of-harmonized-results",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Validation is essential to ensure harmonized data are fit for purpose.\n\n\n\nInternal validation\n\nCross-validation within the source dataset\nBootstrapping to assess model stability\n\nExternal validation\n\nIndependent soil datasets\nBenchmarking against measured values\n\n\n\n\n\nCommonly used statistical indicators include:\n\nBias and Mean Error (ME)\nRoot Mean Square Error (RMSE)\nMean Absolute Error (MAE)\nCoefficient of Determination (R²)\nNash–Sutcliffe Efficiency (NSE)\n\nMetrics should be reported by soil depth and, where possible, by soil class.\n\n\n\n\nResidual plots vs predictors and depth\nSensitivity analysis of input variables\nSpatial autocorrelation of errors"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#documentation-and-reproducibility",
    "href": "use-cases/harmonise-ptf.html#documentation-and-reproducibility",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "For transparency and reuse:\n\nRecord PTF names, equations, or model identifiers\nArchive preprocessing scripts and configuration files\nStore harmonized outputs with uncertainty metadata\nUse version control and persistent identifiers where possible"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#limitations-and-best-practices",
    "href": "use-cases/harmonise-ptf.html#limitations-and-best-practices",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "PTFs are context-dependent and not universal\nHarmonization reduces heterogeneity but does not eliminate uncertainty\nAvoid mixing PTF outputs with direct measurements without clear labeling\nAlways match PTF complexity to data quality and project scale"
  },
  {
    "objectID": "use-cases/harmonise-ptf.html#conclusion",
    "href": "use-cases/harmonise-ptf.html#conclusion",
    "title": "Guidance on Harmonizing Soil Observation Data Using Pedotransfer Functions (PTFs)",
    "section": "",
    "text": "Harmonizing soil observation data using pedotransfer functions is a powerful, but non-trivial, process. Success depends on careful preprocessing, informed PTF selection, rigorous validation, and transparent documentation. When applied responsibly, PTF-based harmonization enables integrated soil analyses across regions, scales, and data sources while preserving scientific credibility."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html",
    "href": "use-cases/inspire-geopackage.html",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "",
    "text": "This strategy outlines an approach for adopting the INSPIRE Good Practice (GP) on GeoPackage (GPKG) as a compliant encoding for soil data. The objective is to enhance interoperability, simplify data exchange, and support the delivery of INSPIRE-compliant datasets in a modern, efficient format. The transition from existing GML-based implementations to GeoPackage aligns with INSPIRE’s recommendations for improving usability and performance in geospatial data delivery."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#inspire-directive-overview",
    "href": "use-cases/inspire-geopackage.html#inspire-directive-overview",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "INSPIRE Directive Overview",
    "text": "INSPIRE Directive Overview\nThe INSPIRE Directive (2007/2/EC) was established to build a European Union spatial data infrastructure that supports environmental policies and activities affecting the environment. Among the 34 data themes listed in Annex III of the Directive, soil is included as one of them. For each theme a common conceptual data model has been suggested, with the aim to be adopted by member states to store and share environmental data. Today the INSPIRE directive is gradually superseded by the regulation on high value datasets. Soil data has been identified as one of the high value datasets."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#good-practice-on-geopackage-encodings",
    "href": "use-cases/inspire-geopackage.html#good-practice-on-geopackage-encodings",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Good Practice on GeoPackage Encodings",
    "text": "Good Practice on GeoPackage Encodings\nThe INSPIRE Good Practice on GeoPackage Encodings describes an encoding for delivering of spatial data which is lighter and more user-friendly than conventional GML encodings. The practice is facilitated by the SQLite-based GeoPackage encoding (OGC standard), which is widely supported by modern GIS software."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#the-inspire-soil-conceptual-data-model",
    "href": "use-cases/inspire-geopackage.html#the-inspire-soil-conceptual-data-model",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "The INSPIRE Soil conceptual data model",
    "text": "The INSPIRE Soil conceptual data model\nThe INSPIRE conceptual data model is based on the Open Geospatial Consortium model for Observations, Measurements and Samples. For each observation made on a feature of interest (a plot, profile or horizon) either in the field or in a laboratory, the model prescribes to capture along with the observed result, also the observed soil property, the observation procedure and the unit of measure."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#assessment-phase",
    "href": "use-cases/inspire-geopackage.html#assessment-phase",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Assessment Phase",
    "text": "Assessment Phase\n\nReview current data holdings and GML-based INSPIRE compliance.\n\nIdentify gaps in encoding capabilities or data model compliance.\n\nAssess tools and workflows currently used for data transformation and publication."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#data-model-alignment",
    "href": "use-cases/inspire-geopackage.html#data-model-alignment",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Data Model Alignment",
    "text": "Data Model Alignment\n\nMap the INSPIRE Soil data model to a schema compatible with GeoPackage.\n\nEnsure the representation of complex features (e.g., nested observations, relationships).\n\nMaintain semantic alignment with INSPIRE data specifications using code lists and controlled vocabularies."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#toolchain-development",
    "href": "use-cases/inspire-geopackage.html#toolchain-development",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Toolchain Development",
    "text": "Toolchain Development\n\nSelect or develop tools for transforming existing datasets into GeoPackage format.\n\nAutomate validation workflows using tools such as ETF Validator or HALE Studio.\n\nEnsure export scripts support relevant feature types and attribute mappings."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#pilot-and-testing",
    "href": "use-cases/inspire-geopackage.html#pilot-and-testing",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Pilot and Testing",
    "text": "Pilot and Testing\n\nProduce a pilot GeoPackage dataset for selected regions or features.\n\nValidate data encoding using INSPIRE ETF testing services.\n\nGather feedback from internal and external stakeholders."
  },
  {
    "objectID": "use-cases/inspire-geopackage.html#full-roll-out",
    "href": "use-cases/inspire-geopackage.html#full-roll-out",
    "title": "Strategy Document for Adopting the INSPIRE Good Practice on GeoPackage for the Soil Data Theme",
    "section": "Full Roll-Out",
    "text": "Full Roll-Out\n\nImplement full-scale transformation of Soil data to GeoPackage.\n\nUpdate metadata to reflect the new encoding format.\n\nPublish datasets via download services in accordance with INSPIRE TG requirements."
  },
  {
    "objectID": "use-cases/knowledge-hubs-and-websites.html",
    "href": "use-cases/knowledge-hubs-and-websites.html",
    "title": "Strategy for Adopting FAIR Principles in Project Websites and Knowledge Hubs",
    "section": "",
    "text": "The FAIR principles (Findability, Accessibility, Interoperability, and Reusability) are essential guidelines for enhancing the utility of digital assets. This strategy outlines how to integrate these principles into the development, maintenance, and archiving of project websites and data and/or knowledge hubs, which are developed as part of a research project. Central in this strategy is to establish robust archiving practices for long-term content preservation, because due to their nature, project websites and knowledge hubs have a high likelihood of content alteration and discontinuation.\nProject websites and especially knowledge hubs developed in large Horizon Europe projects have a tendency to contain unique information which is not available elsewhere on the web. Due to the project timeline, the websites and hubs tend to be unmaintained after the project, handed over to a second party, abandoned or discontinued. If not carefully archived this unique information has a high risk of being lost. The project websites are ofter referenced from journal articles and (progress) reports. A progress report typically aims to reference a specific state of the deliverable, which is difficult if the content of the system is dynamic. For both scenario’s, it is of interest to persist the state of the system at intervals, but at least at project finalisation, to facilitate consistent access to the content and accurate citation.\nThis document provides a strategy on how to facilitate a process in which the content of an online system is operated and archived following the FAIR principles."
  },
  {
    "objectID": "use-cases/knowledge-hubs-and-websites.html#metadata-oriented-knowledge-hubs",
    "href": "use-cases/knowledge-hubs-and-websites.html#metadata-oriented-knowledge-hubs",
    "title": "Strategy for Adopting FAIR Principles in Project Websites and Knowledge Hubs",
    "section": "Metadata oriented knowledge hubs",
    "text": "Metadata oriented knowledge hubs\nIf a knowledge and/or data hub is oriented on metadata descring a set of resources, the final archive of the content will benefit from a structured format using a standardised ontology, such as Dublin Core. Consider for example a CSV format with each row describing a resource, with metadata fields as columns (title, abstract, subject, license, creator, date). In this scenario you can use the CSVW approach in the use case tabular data to describe the CSV in an interoperable way."
  },
  {
    "objectID": "use-cases/spectroscopy.html",
    "href": "use-cases/spectroscopy.html",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "This document outlines best practices for storing, documenting, and referencing soil sample observation data acquired via spectrometry techniques. It addresses two common workflows:\n\nCalibration campaigns, where a subset of samples is analyzed using both wet chemistry and spectrometry to build or expand a spectral library.\nPrediction campaigns, where only spectrometry is performed and an existing spectral library is used for inference.\n\nIn both cases, spectral libraries must be traceable, documented, and referenced correctly by each spectral observation.\n\n\n\n\n\nEach physical soil sample should be represented as a unique entity with:\n\nSample ID (persistent, unique identifier)\nCollection metadata:\n\nGeographic location (coordinates, site name)\nDepth, horizon, or core information\nDate and method of collection\nCollector or organization\n\nStorage and handling information (if relevant)\n\n\n\n\nEach spectrometry measurement should be stored as its own record:\n\nLink to Sample ID\nSpectrometer/device ID and model\nWavelength range and resolution\nAcquisition settings (e.g. gain, integration time, replicates)\nDate of measurement and operator\nPreprocessing steps (e.g. smoothing, normalization)\nReference to the spectral library used for calibration or prediction\n\n\n\n\nWhen performed:\n\nLink to Sample ID\nAnalytes measured (e.g. SOC, pH, nutrients, texture)\nLaboratory method and protocols (e.g. ISO, USDA, local standards)\nLaboratory or institution\nUnits and uncertainty\nDate of analysis, analyst or laboratory code\n\n\n\n\n\n\n\nA spectral library is a curated dataset that includes:\n\nSpectral observations linked to samples with known reference values (typically from wet chemistry)\nCalibration models derived from those paired data\nPotential metadata on environmental or soil type ranges\n\n\n\n\nEach spectral library should have its own identifier and descriptive record with:\n\nLibrary ID (unique identifier)\nPurpose (e.g. SOC prediction, multi-property calibration)\nScope (geographical region, soil types, date range)\nDevice(s) used\nPreprocessing and modeling approach (e.g. PLSR, machine learning)\nVersioning details and modification history\nData quality criteria and validation metrics\nContributors/maintainers and contact details\nLink to the paired wet chemistry dataset used to build or update it\n\n\n\n\n\n\n\nWhen a sample subset undergoes both wet chemistry and spectrometry:\n\nEach spectral observation must be tagged with:\n\nThe Library ID it contributes to or helps calibrate\nWhether it is part of calibration, validation, or test sets\n\nWet chemistry records should be clearly tied to corresponding spectral records, using the Sample ID.\n\n\n\n\nWhen only spectrometry is performed:\n\nEach spectral sample must reference the Library ID (and version) used to generate predictions.\nStore predicted values separately but linked to:\n\nThe Sample ID\nThe spectral observation record\nThe specific model version within the library\n\n\n\n\n\n\n\n\nUse a structured and queryable system (e.g. relational database, standardized file formats with metadata). At minimum, maintain:\n\nSamples Table Sample ID, collection metadata.\nSpectral Observations Table Spectral file reference, Sample ID, device metadata, spectral library reference.\nWet Chemistry Table Sample ID, analytes, lab methods, values, units.\nSpectral Library Table Library ID, metadata, versioning, reference to calibration data.\nModel or Prediction Table (if applicable) Prediction target, model version, linked spectral observation.\n\n\n\n\n\nSpectral data: e.g. CSV, JSON, ENVI files, or binary vendor formats with metadata sidecar files.\nMetadata: embed or link in machine-readable form (e.g. YAML, JSON).\nSpectral library bundles: zip/TAR structures including documentation and metadata file.\n\nEnsure every file contains or links to identifiers for:\n\nSample ID\nSpectral observation ID\nLibrary ID\n\n\n\n\n\n\nAssign version numbers to spectral libraries and calibration models.\nNever overwrite older versions—archive instead.\nRecord provenance:\n\nWho created or updated a library\nDate and reason for changes\nData or models added, removed, or recalibrated\n\n\n\n\n\n\n\n\nDocument selection criteria for calibration and validation samples.\nStore cross-validation metrics (e.g. RMSE, R², residuals).\nFlag outliers or questionable measurements.\n\n\n\n\n\nClearly separate predicted values from measured ones.\nTrack confidence intervals or uncertainty estimates.\nEnsure the model’s applicability domain is documented.\n\n\n\n\n\nCreate or store documentation for:\n\nData schemas\nMetadata standards used (e.g. INSPIRE, ISO 19115, OGC)\nNaming conventions and identifiers\nData access protocols (e.g. APIs, shared drives, repositories)\n\nWhere appropriate:\n\nUse DOIs or persistent links for spectral libraries.\nProvide clear citation formats.\n\n\n\n\nSince new campaigns may rely entirely on existing libraries:\n\nRequire each new spectral observation to:\n\nReference the library used for prediction\nInclude version information\nRecord whether any local calibration updates were applied\n\nIf new wet chemistry is added later, update the library accordingly with a new version and document the change.\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nMust Include\nPurpose\n\n\n\n\nSample Record\nSample ID, collection data\nTraceability\n\n\nSpectral Observation\nSample ID, device metadata, spectral file, Library ID\nReusability & reference\n\n\nWet Chemistry Record\nSample ID, lab methods, analyte values\nCalibration & validation\n\n\nSpectral Library\nLibrary ID, metadata, version, scope, provenance\nPrediction & documentation\n\n\nModel/Prediction Link\nObservation ID, Library ID, model version, predicted values\nTransparency\n\n\n\n\n\n\n\nAlign with FAIR principles (Findable, Accessible, Interoperable, Reusable).\nUse persistent identifiers and open metadata standards.\nPlan for interoperability across institutions, countries, and legacy datasets."
  },
  {
    "objectID": "use-cases/spectroscopy.html#purpose",
    "href": "use-cases/spectroscopy.html#purpose",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "This document outlines best practices for storing, documenting, and referencing soil sample observation data acquired via spectrometry techniques. It addresses two common workflows:\n\nCalibration campaigns, where a subset of samples is analyzed using both wet chemistry and spectrometry to build or expand a spectral library.\nPrediction campaigns, where only spectrometry is performed and an existing spectral library is used for inference.\n\nIn both cases, spectral libraries must be traceable, documented, and referenced correctly by each spectral observation."
  },
  {
    "objectID": "use-cases/spectroscopy.html#core-data-entities",
    "href": "use-cases/spectroscopy.html#core-data-entities",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Each physical soil sample should be represented as a unique entity with:\n\nSample ID (persistent, unique identifier)\nCollection metadata:\n\nGeographic location (coordinates, site name)\nDepth, horizon, or core information\nDate and method of collection\nCollector or organization\n\nStorage and handling information (if relevant)\n\n\n\n\nEach spectrometry measurement should be stored as its own record:\n\nLink to Sample ID\nSpectrometer/device ID and model\nWavelength range and resolution\nAcquisition settings (e.g. gain, integration time, replicates)\nDate of measurement and operator\nPreprocessing steps (e.g. smoothing, normalization)\nReference to the spectral library used for calibration or prediction\n\n\n\n\nWhen performed:\n\nLink to Sample ID\nAnalytes measured (e.g. SOC, pH, nutrients, texture)\nLaboratory method and protocols (e.g. ISO, USDA, local standards)\nLaboratory or institution\nUnits and uncertainty\nDate of analysis, analyst or laboratory code"
  },
  {
    "objectID": "use-cases/spectroscopy.html#spectral-libraries",
    "href": "use-cases/spectroscopy.html#spectral-libraries",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "A spectral library is a curated dataset that includes:\n\nSpectral observations linked to samples with known reference values (typically from wet chemistry)\nCalibration models derived from those paired data\nPotential metadata on environmental or soil type ranges\n\n\n\n\nEach spectral library should have its own identifier and descriptive record with:\n\nLibrary ID (unique identifier)\nPurpose (e.g. SOC prediction, multi-property calibration)\nScope (geographical region, soil types, date range)\nDevice(s) used\nPreprocessing and modeling approach (e.g. PLSR, machine learning)\nVersioning details and modification history\nData quality criteria and validation metrics\nContributors/maintainers and contact details\nLink to the paired wet chemistry dataset used to build or update it"
  },
  {
    "objectID": "use-cases/spectroscopy.html#referencing-spectral-libraries",
    "href": "use-cases/spectroscopy.html#referencing-spectral-libraries",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "When a sample subset undergoes both wet chemistry and spectrometry:\n\nEach spectral observation must be tagged with:\n\nThe Library ID it contributes to or helps calibrate\nWhether it is part of calibration, validation, or test sets\n\nWet chemistry records should be clearly tied to corresponding spectral records, using the Sample ID.\n\n\n\n\nWhen only spectrometry is performed:\n\nEach spectral sample must reference the Library ID (and version) used to generate predictions.\nStore predicted values separately but linked to:\n\nThe Sample ID\nThe spectral observation record\nThe specific model version within the library"
  },
  {
    "objectID": "use-cases/spectroscopy.html#data-persistence-and-storage-structure",
    "href": "use-cases/spectroscopy.html#data-persistence-and-storage-structure",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Use a structured and queryable system (e.g. relational database, standardized file formats with metadata). At minimum, maintain:\n\nSamples Table Sample ID, collection metadata.\nSpectral Observations Table Spectral file reference, Sample ID, device metadata, spectral library reference.\nWet Chemistry Table Sample ID, analytes, lab methods, values, units.\nSpectral Library Table Library ID, metadata, versioning, reference to calibration data.\nModel or Prediction Table (if applicable) Prediction target, model version, linked spectral observation.\n\n\n\n\n\nSpectral data: e.g. CSV, JSON, ENVI files, or binary vendor formats with metadata sidecar files.\nMetadata: embed or link in machine-readable form (e.g. YAML, JSON).\nSpectral library bundles: zip/TAR structures including documentation and metadata file.\n\nEnsure every file contains or links to identifiers for:\n\nSample ID\nSpectral observation ID\nLibrary ID"
  },
  {
    "objectID": "use-cases/spectroscopy.html#versioning-and-traceability",
    "href": "use-cases/spectroscopy.html#versioning-and-traceability",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Assign version numbers to spectral libraries and calibration models.\nNever overwrite older versions—archive instead.\nRecord provenance:\n\nWho created or updated a library\nDate and reason for changes\nData or models added, removed, or recalibrated"
  },
  {
    "objectID": "use-cases/spectroscopy.html#quality-control-and-validation",
    "href": "use-cases/spectroscopy.html#quality-control-and-validation",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Document selection criteria for calibration and validation samples.\nStore cross-validation metrics (e.g. RMSE, R², residuals).\nFlag outliers or questionable measurements.\n\n\n\n\n\nClearly separate predicted values from measured ones.\nTrack confidence intervals or uncertainty estimates.\nEnsure the model’s applicability domain is documented."
  },
  {
    "objectID": "use-cases/spectroscopy.html#documentation-and-accessibility",
    "href": "use-cases/spectroscopy.html#documentation-and-accessibility",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Create or store documentation for:\n\nData schemas\nMetadata standards used (e.g. INSPIRE, ISO 19115, OGC)\nNaming conventions and identifiers\nData access protocols (e.g. APIs, shared drives, repositories)\n\nWhere appropriate:\n\nUse DOIs or persistent links for spectral libraries.\nProvide clear citation formats."
  },
  {
    "objectID": "use-cases/spectroscopy.html#linking-campaigns-through-libraries",
    "href": "use-cases/spectroscopy.html#linking-campaigns-through-libraries",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Since new campaigns may rely entirely on existing libraries:\n\nRequire each new spectral observation to:\n\nReference the library used for prediction\nInclude version information\nRecord whether any local calibration updates were applied\n\nIf new wet chemistry is added later, update the library accordingly with a new version and document the change."
  },
  {
    "objectID": "use-cases/spectroscopy.html#summary-of-key-requirements",
    "href": "use-cases/spectroscopy.html#summary-of-key-requirements",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Element\nMust Include\nPurpose\n\n\n\n\nSample Record\nSample ID, collection data\nTraceability\n\n\nSpectral Observation\nSample ID, device metadata, spectral file, Library ID\nReusability & reference\n\n\nWet Chemistry Record\nSample ID, lab methods, analyte values\nCalibration & validation\n\n\nSpectral Library\nLibrary ID, metadata, version, scope, provenance\nPrediction & documentation\n\n\nModel/Prediction Link\nObservation ID, Library ID, model version, predicted values\nTransparency"
  },
  {
    "objectID": "use-cases/spectroscopy.html#compliance-and-future-proofing",
    "href": "use-cases/spectroscopy.html#compliance-and-future-proofing",
    "title": "Guidance for Persisting Soil Sample Observation Data from Spectrometry and Wet Chemistry",
    "section": "",
    "text": "Align with FAIR principles (Findable, Accessible, Interoperable, Reusable).\nUse persistent identifiers and open metadata standards.\nPlan for interoperability across institutions, countries, and legacy datasets."
  },
  {
    "objectID": "use-cases/vocabularies.html",
    "href": "use-cases/vocabularies.html",
    "title": "Strategy Document for the Creation and Adoption of Vocabularies in Soil Mission Research Projects",
    "section": "",
    "text": "Introduction\nThe effective use of vocabularies and glossaries is critical in Horizon Europe research projects to ensure interoperability, clarity, and reuse of knowledge. This strategy outlines best practices for the creation, adoption, and maintenance of vocabularies throughout the project lifecycle.\n\n\nWhy Agree on a Vocabulary at the Start of the Project\n\nCommon Understanding: Establishing a shared vocabulary at the project outset fosters a mutual understanding among diverse stakeholders.\n\nImproved Collaboration: A clear glossary reduces ambiguity and enhances communication across disciplines, countries, and institutions.\n\nData Interoperability: Agreeing on standard terms supports seamless data integration, especially in multi-source datasets.\n\nKnowledge Retention: It aids in documentation, ensuring that terms and their meanings are preserved for future reference and impact assessment.\n\n\n\nCharacteristics of Effective Vocabularies\n\nUniquely Identifiable Terms: Each concept or term should be assigned a persistent, globally unique identifier (e.g., URI) to avoid confusion and enable precise referencing.\n\nRich Semantic Relationships: Terms should be connected through relations with eligible vocabularies, such as AgroVoc, Gemet, EuroVoc, ISO11074 via relations such as:\n\nskos:exactMatch: Indicates that two terms can be used interchangeably.\n\nskos:closeMatch: Suggests the terms are similar but not interchangeable in all contexts.\n\nskos:broader: Shows a hierarchical relationship where one term is more general.\n\nskos:narrower: Shows a more specific concept within a broader term.\n\nAlignment with Existing Standards: Reuse and align with established vocabularies (e.g., SKOS, Dublin Core, schema.org) whenever possible.\n\n\n\nSteps for Vocabulary Management\n\nInitial Scoping: Identify key domains and stakeholders.\n\nTerm Collection: Gather terms from partners, existing ontologies, and relevant documentation.\n\nConsensus Building: Facilitate workshops to agree on preferred terms and definitions.\n\nModeling and Encoding: Represent the vocabulary using semantic web standards (e.g., RDF, SKOS).\n\nPublishing: Publish the vocabulary on a persistent, accessible platform with both human-readable and machine-readable views. Register the published vocabulary in vocabulary catalogues, such as\n\nMaintenance: Regularly review and update the vocabulary to reflect project developments.\n\n\n\nTerms collection in a spreadsheet\nSpreadsheets are an effective tool to collect and discuss terms with contributors. The spreadsheet is either hosted on a shared environment (sharepoint, google docs) with multiple editors, or hosted as a CSV in Github (contributions added via pull requests).\nA typical structure for the spreadsheet is:\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\npreflabel\naltlabel\ndefinition\nExact- match\nClose-match\nbroader\nnarrower\n\n\n\n\ntree\ntree\ntree crop\nwoody perennials\nhttp://aims.fao.org/aos/agrovoc/c_34134\n#wood, #timber\n#forest\n#oak, #spruce, #pine\n\n\n\nLabels are classified as the preferred label and alternative labels (synonyms). The 4 last columns indicate relationships between concepts, either relations within the document (indicated with #), but it can also have links to remote vocabularies.\nLabels and Definition can be captured in various languages, either as extra columns or in a separate spreadsheet\n\n\n\nID\npreflabel\naltlabel\ndefinition\nlanguage\n\n\n\n\ntree\ntree\n\n\nen\n\n\ntree\narbre\n\n\nfr\n\n\n\n\n\nProvide a machine readable interface\nSimple Knowledge Organization System (SKOS) is a commonly used ontology to organise concepts their definitions and relations. Various guidances and tools exist which are able to produce a SKOS concept scheme from a spreadsheet. The result is a json-ld, xml/rdf or turtle file, which can be deposited at a vocabulary repository, such as\nConcepts are identified by an identifier which preferably resolve to the definition of the concept, when used as web address in a browser or data tool. The prefix of the identifier (https://example.com/vocab#) references the actual document or a proxy is placed in between, which forwards users to where the document is hosted.\nCommonly available proxy solutions are w3id.org and doi.org.\nVocabulary repositories such as AgroPortal typically facilitate the full process of deposit and handling of concept identifiers.\n\n\nProviding an HTML Interface for Human Browsing\nTo ensure accessibility and usability of the vocabulary by non-technical users, an HTML interface should be developed. This interface should:\n\nPresent vocabulary entries in an intuitive, searchable, and hierarchical format.\n\nAllow users to browse terms, view definitions, and explore semantic relationships (broader, narrower, exact match, etc.).\n\nInclude filtering, keyword search, and alphabetic navigation.\n\nSupport multilingual labels and definitions if applicable.\n\nBe responsive and accessible across devices.\n\nRecommended approaches include:\n\nUsing tools such as Skosmos, Widoco or VocView to render SKOS vocabularies as user-friendly HTML pages.\n\nEmbedding RDFa or JSON-LD in the HTML to maintain machine-readability alongside human-readability.\n\nIf proxy software is used, the proxy can be configured to identify if the visiting client is a web browser or a machine, and forward the client to the relevant representation of the concept.\nThis mechanism is used by Glosis Web Ontology. Depending on the type of client a request to http://w3id.org/glosis/model/layerhorizon/ManganeseExtractableElements is forwarded to the relevant location.\n\n\nConclusion\nAgreeing on and managing vocabularies is foundational to the success of Horizon Europe research projects. By ensuring consistency, discoverability, and interoperability of terms, research outputs become more usable and impactful within and beyond the project consortium."
  }
]